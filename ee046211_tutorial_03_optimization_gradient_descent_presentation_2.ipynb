{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <img src=\"./assets/course-icon.png\" style=\"height:50px;display:inline\"> ECE 046211 - Technion - Deep Learning\n",
    "---\n",
    "\n",
    "#### <a href=\"https://taldatech.guthub.io\"> Tal Daniel</a>\n",
    "\n",
    "## Tutorial 03 - Optimization & Gradient Descent Algorithms\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/checklist.png\" style=\"height:50px;display:inline\"> Agenda\n",
    "---\n",
    "\n",
    "<div style=\"width:100%;height:800px;line-height:3em;overflow:scroll;padding:5px;\">\n",
    "\n",
    "* [Unimodal vs. Multimodal Optimization](#-Unimodal-vs.-Multimodal-Optimization)\n",
    "* [Convexity](#-Convexity)\n",
    "* [Optimality Conditions](#-Optimality-Conditions)\n",
    "* [(Batch) Gradient Descent](#(Batch)-Gradient-Descent)\n",
    "* [Stochastic Gradient Descent (Mini Batch Gradient Descent)](#Stochastic-Gradient-Descent-(Mini-Batch-Gradient-Descent))\n",
    "* [GD Comparison Summary](#GD-Comparison-Summary)\n",
    "* [The Learning Rate](#-The-Learning-Rate)\n",
    "* [Example - (Multivariate) Linear Least Squares](#-Example---(Multivariate)-Linear-Least-Squares)\n",
    "* [Learning Rate Scheduling (Annealing)](#-Learning-Rate-Scheduling-(Annealing))\n",
    "    * [Learning Rate Scheduling in PyTorch](#-Learning-Rate-Scheduling-in-PyTorch)\n",
    "* [Momentum & Nesterov Momentum](#-Momentum-&-Nesterov-Momentum)\n",
    "    * [Momentum in PyTorch](#-Momentum-in-PyTorch)\n",
    "* [Adaptive Learning Rate Methods](#-Adaptive-Learning-Rate-Methods)\n",
    "    * [Adagrad](#-Adagrad)\n",
    "        * [Adagrad in PyTorch](#-Adagrad-in-PyTorch)\n",
    "    * [RMSprop](#-RMSprop)\n",
    "        * [RMSprop in PyTorch](#-RMSprop-in-PyTorch)\n",
    "    * [Adam - Adaptive Moment Estimation](#-Adam---Adaptive-Moment-Estimation)\n",
    "        * [Adam in PyTorch](#-Adam-in-PyTorch)\n",
    "* [Comparison Between Methods](#-Comparison-Between-Methods)\n",
    "* [Recent Advances in Optimizers](#-Recent-Advances-in-Optimizers)\n",
    "    * [AdaBelief](#-AdaBelief)\n",
    "    * [MADGRAD](#-MADGRAD)\n",
    "    * [Adan](#-Adan)\n",
    "    * [Schedule-free Optimization](#-Schedule-Free-Optimization)\n",
    "\n",
    "* [Recommended Videos](#-Recommended-Videos)\n",
    "* [Credits](#-Credits)</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<!-- ### <img src=\"https://img.icons8.com/bubbles/50/000000/checklist.png\" style=\"height:50px;display:inline\"> Agenda\n",
    "---\n",
    "\n",
    "* [Unimodal vs. Multimodal Optimization](#-Unimodal-vs.-Multimodal-Optimization)\n",
    "* [Convexity](#-Convexity)\n",
    "* [Optimality Conditions](#-Optimality-Conditions)\n",
    "* [(Batch) Gradient Descent](#(Batch)-Gradient-Descent)\n",
    "* [Stochastic Gradient Descent (Mini Batch Gradient Descent)](#Stochastic-Gradient-Descent-(Mini-Batch-Gradient-Descent))\n",
    "* [GD Comparison Summary](#GD-Comparison-Summary)\n",
    "* [The Learning Rate](#-The-Learning-Rate)\n",
    "* [Example - (Multivariate) Linear Least Squares](#-Example---(Multivariate)-Linear-Least-Squares)\n",
    "* [Learning Rate Scheduling (Annealing)](#-Learning-Rate-Scheduling-(Annealing))\n",
    "    * [Learning Rate Scheduling in PyTorch](#-Learning-Rate-Scheduling-in-PyTorch)\n",
    "* [Momentum & Nesterov Momentum](#-Momentum-&-Nesterov-Momentum)\n",
    "    * [Momentum in PyTorch](#-Momentum-in-PyTorch)\n",
    "* [Adaptive Learning Rate Methods](#-Adaptive-Learning-Rate-Methods)\n",
    "    * [Adagrad](#-Adagrad)\n",
    "        * [Adagrad in PyTorch](#-Adagrad-in-PyTorch)\n",
    "    * [RMSprop](#-RMSprop)\n",
    "        * [RMSprop in PyTorch](#-RMSprop-in-PyTorch)\n",
    "    * [Adam - Adaptive Moment Estimation](#-Adam---Adaptive-Moment-Estimation)\n",
    "        * [Adam in PyTorch](#-Adam-in-PyTorch)\n",
    "* [Comparison Between Methods](#-Comparison-Between-Methods)\n",
    "* [Recent Advances in Optimizers](#-Recent-Advances-in-Optimizers)\n",
    "    * [AdaBelief](#-AdaBelief)\n",
    "    * [MADGRAD](#-MADGRAD)\n",
    "    * [Adan](#-Adan)\n",
    "    * [Schedule-free Optimization](#-Schedule-Free-Optimization)\n",
    "\n",
    "* [Recommended Videos](#-Recommended-Videos)\n",
    "* [Credits](#-Credits) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# imports for the tutorial\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.model_selection import train_test_split\n",
    "# %matplotlib ipympl\n",
    "%matplotlib inline\n",
    "\n",
    "# pytorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Unimodal vs. Multimodal Optimization\n",
    "---\n",
    "* **Unimodal** - only **one** optimum, that is, the *local* optimum is also global.\n",
    "<center><img src=\"./assets/unimodal.jpg\" style=\"height:190px\"></center>\n",
    "\n",
    "* **Multimodal** - more than one optimum.\n",
    "<center><img src=\"./assets/multimodal.jpg\" style=\"height:200px\"></center>\n",
    "\n",
    "Most search schemes are based on the assumption of **unimodal** surface. The optimum determined in such cases is called **local optimum design**.\n",
    "\n",
    "The **global optimum** is the best of all *local optimum* designs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Convexity\n",
    "---\n",
    "<div style=\"width:100%;height:800px;line-height:3em;overflow:scroll;padding:5px;\">\n",
    "    \n",
    "* **Definition**: $ f: X \\rightarrow \\mathbb{R}$ is a convex function if\n",
    "$$\\forall x_1, x_2 \\in X, \\forall t \\in [0,1]: $$ $$ f(tx_1 + (1-t)x_2) \\leq tf(x_1) + (1-t)f(x_2) $$\n",
    "  \n",
    "<center><img src=\"./assets/convex_1.jpg\" style=\"height:200px\"></center>\n",
    "\n",
    "* Convex Set: \n",
    "$$\\forall x_1, x_2 \\in X, \\forall t \\in [0,1]: $$ $$ tx_1 + (1-t)x_2 \\in X $$\n",
    "\n",
    "<center><img src=\"./assets/convex_concave.gif\" style=\"height:200px\"></center>\n",
    "\n",
    "<!-- <a href=\"http://mathworld.wolfram.com/Convex.html\">Image Source</a> -->\n",
    "* Convex functions are **unimodal**.\n",
    "* Unimodal functions are not always convex, but they are usually still easy to optimize.\n",
    "      \n",
    "<center><img src=\"./assets/convex_2.jpg\" style=\"height:200px\"></center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimality Conditions\n",
    "---\n",
    "<div style=\"width:100%;height:800px;line-height:3em;overflow:scroll;padding:5px;\">\n",
    "\n",
    "* If $f: \\mathbb{R}^d \\to \\mathbb{R}$ has *local* optimum at $x_0$ then $\\nabla f(x_0) = 0$. \n",
    "<!--     * $\\nabla f(x_0)$ is also called **the gradient** at $x_0$. -->\n",
    "* **The Hessian Matrix** : $H(f)(x)_{i,j} = \\frac{\\partial^2}{\\partial x_i \\partial x_j} f(x) \\in \\mathbb{R}^{d \\times d}$\n",
    "$$ H = \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_d}  \\\\ \\frac{\\partial^2 f}{\\partial x_2 x_1} & \\frac{\\partial^2 f}{\\partial x_2^2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_2 \\partial x_d}  \\\\ \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\ \\frac{\\partial^2 f}{\\partial x_d x_1} & \\frac{\\partial^2 f}{\\partial x_d \\partial x_2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_d^2}  \\end{bmatrix} $$\n",
    "\n",
    "* If the **Hessian** matrix is:\n",
    "    * **Positive Definite** (all eigenvalues *positive*) at $x_0 \\rightarrow$ *local minimum*.\n",
    "    * **Negative Definite** (all eigenvalues *negative*) at $x_0 \\rightarrow$ *local maximum*.\n",
    "    * Both **positive and negative** eigenvalues at $x_0 \\rightarrow$ *saddle* point.\n",
    "      <center><img src=\"./assets/saddle.jpg\" style=\"height:100px\"></center>\n",
    "* Note: the Hessian matrix is symmetric if the second partial derivatives are continuous, but this is not always true (<a href=\"https://en.wikipedia.org/wiki/Symmetry_of_second_derivatives\">Schwarz's theorem</a>).\n",
    "\n",
    "* **1D Case**:  \n",
    "    If $f: \\mathbb{R} \\to \\mathbb{R}$ and $f''(x_0)$ exists:\n",
    "    * If $f''(x_0) > 0$, then $x_0$ is a *local minimum*.\n",
    "    * If $f''(x_0) < 0$, then $x_0$ is a *local maximum*.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <img src=\"https://img.icons8.com/dusk/64/000000/treasure-map.png\" style=\"height:50px;display:inline\">(Batch) Gradient Descent\n",
    "---\n",
    "<div style=\"width:100%;height:800px;line-height:3em;overflow:scroll;padding:5px;\">\n",
    "\n",
    "* Generic optimization algorithm capable of finding optimal solutions to a wide range of problems.\n",
    "* The general idea is to tweak parameters **iteratively** to minimize a cost function.\n",
    "* It measures the local gradient of the error function with regards to the parameter vector ($\\theta$ or $w$), and it goes down in the direction of the descending gradient.\n",
    "* **Once the gradient is zero - the minimum is reached (=convergence)**.\n",
    "\n",
    "* **Pseudocode**:\n",
    "    * **Require**: Learning rate $\\alpha_k$\n",
    "    * **Require**: Initial parameter vector $w$\n",
    "    * **While** stopping criterion not met **do**\n",
    "        * Compute gradient: $g \\leftarrow \\nabla_w f(x,w)$ \n",
    "        * Apply update: $w \\leftarrow w - \\alpha_k g$\n",
    "        * $k \\leftarrow k + 1$\n",
    "    * **end while**\n",
    "\n",
    "<center><img src=\"./assets/minimum.jpg\" style=\"height:400px\"></center>\n",
    "\n",
    "**Convergence Gurantee**:\n",
    "When \n",
    "1. The cost function is <ins>convex</ins> and \n",
    "2. Its slope <ins>does not change abruptly</ins>\n",
    "\n",
    "(Batch) GD with a (small enough) *fixed* learning rate will eventually converge to the optimal solution (but the time is depndent on the rate).\n",
    "\n",
    "<center><img src=\"./assets/gd_animation.gif\" style=\"height:400px\"></center>\n",
    "\n",
    "<a href=\"https://commons.wikimedia.org/wiki/File:Gradient_descent.gif\">Image Source</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<!-- ### <img src=\"https://img.icons8.com/dusk/64/000000/waypoint-map.png\" style=\"height:50px;display:inline\">Stochastic Gradient Descent (Mini-Batch Gradient Descent)\n",
    "---\n",
    "* The main problem with (Batch) GD is that it uses the **whole** training set to compute the gradients. But what if that training set is huge or each sample has a very large number of features? Computing the gradient can take a very long time.\n",
    "* *Stochastic* Gradient Descent on the other hand, samples just one instance randomly at every step and computes the gradients based on that single instance (remember the Perceptron algorithm?). This makes the algorithm much faster but due to its randomness, it is much less stable. Instead of steady decreasing untill reaching the minimum, the cost function will bounce up and down, **decreasing only on average**. With time, it will get *very close* to the minimum, but once it is there it will continue to bounce around!\n",
    "    * When we have outliers, SGD may take us to undesirable areas.\n",
    "* The final parameters are good but **not optimal**. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<!-- * When the cost function is very irregular, this bouncing can actually **help the algorithm escape local minima**, so SGD has better chance to find the *global* minimum.\n",
    "* How to find optimal parameters using SGD?\n",
    "    * **Reduce the learning rate gradually**: this is called *learning rate scheduling*.\n",
    "        * But don't reduce too quickly or you will get stuck at a local minimum or even frozen!\n",
    "* *Mini-Batch* Gradient Descent - same idea as SGD, but instead of one instance each step, $m$ samples.\n",
    "    * Get a little bit closer to the minimum than SGD but a little harder to escape local minima. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <img src=\"https://img.icons8.com/dusk/64/000000/waypoint-map.png\" style=\"height:50px;display:inline\">Stochastic Gradient Descent (Mini-Batch Gradient Descent)\n",
    "---\n",
    "<div style=\"width:100%;height:800px;line-height:3em;overflow:scroll;padding:5px;\">\n",
    "\n",
    "- **Batch Gradient Descent (GD)**: Computes gradients using the <ins>entire dataset</ins> - very expensive!\n",
    "- **Stochastic Gradient Descent (SGD)**:  \n",
    "  - Uses a single random sample per step, making it faster but less stable.  \n",
    "  - Cost function *fluctuates* but <ins>decreases on average</ins>.\n",
    "      - **+**  Helpful in escaping local minima.  \n",
    "      - **-**  Seneitive to outliers.  \n",
    "  - Solution: *Learning rate scheduling* - gradually reducing the learning rate over time.  \n",
    "- **Mini-Batch Gradient Descent**:  \n",
    "  - Uses a small batch of *m* samples per step.  \n",
    "  - Balances efficiency (like SGD) with reduced fluctuations, improving convergence stability.\n",
    "* **Pseudocode**:\n",
    "    * **Require**: Learning rate $\\alpha_k$\n",
    "    * **Require**: Initial parameter $w$\n",
    "    * **While** stopping criterion not met **do**\n",
    "        * Sample a minibatch of $m$ examples from the training set ($m=1$ for SGD)\n",
    "        * Set $\\{x_1,...,x_m,\\}$ with corresponding targets $\\{y_1,...,y_m\\}$\n",
    "        * Compute gradient: $g \\leftarrow \\frac{1}{m} \\sum_{i=1}^m f'(x_i,w, y_i)$\n",
    "        * Apply update: $w \\leftarrow w - \\alpha_k g$\n",
    "        * $k \\leftarrow k + 1$\n",
    "    * **end while**\n",
    "<center><img src=\"./assets/sgd.png\" style=\"height:400px\"></center>\n",
    "\n",
    "<a href=\"https://towardsdatascience.com/gradient-descent-algorithm-and-its-variants-10f652806a3\">Image Source</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<!-- ### <img src=\"https://img.icons8.com/dusk/64/000000/waypoint-map.png\" style=\"height:50px;display:inline\">Stochastic Gradient Descent (Mini-Batch Gradient Descent)\n",
    "---\n",
    "- **Batch Gradient Descent (GD)**: Computes gradients using the <ins>entire dataset</ins> - very expensive!\n",
    "- **Stochastic Gradient Descent (SGD)**:  \n",
    "  - Uses a single random sample per step, making it faster but less stable.  \n",
    "  - Cost function *fluctuates* but <ins>decreases on average</ins>.\n",
    "      - **+**  Helpful in escaping local minima.  \n",
    "      - **-**  Seneitive to outliers.  \n",
    "  - Solution: *Learning rate scheduling* - gradually reducing the learning rate over time.  \n",
    "- **Mini-Batch Gradient Descent**:  \n",
    "  - Uses a small batch of *m* samples per step.  \n",
    "  - Balances efficiency (like SGD) with reduced fluctuations, improving convergence stability.   -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<!-- * **Pseudocode**:\n",
    "    * **Require**: Learning rate $\\alpha_k$\n",
    "    * **Require**: Initial parameter $w$\n",
    "    * **While** stopping criterion not met **do**\n",
    "        * Sample a minibatch of $m$ examples from the training set ($m=1$ for SGD)\n",
    "        * Set $\\{x_1,...,x_m,\\}$ with corresponding targets $\\{y_1,...,y_m\\}$\n",
    "        * Compute gradient: $g \\leftarrow \\frac{1}{m} \\sum_{i=1}^m f'(x_i,w, y_i)$\n",
    "        * Apply update: $w \\leftarrow w - \\alpha_k g$\n",
    "        * $k \\leftarrow k + 1$\n",
    "    * **end while** -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<!-- <center><img src=\"./assets/sgd.png\" style=\"height:250px\"></center>\n",
    "\n",
    "<a href=\"https://towardsdatascience.com/gradient-descent-algorithm-and-its-variants-10f652806a3\">Image Source</a> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Update Speed\n",
    "---\n",
    "- Example: \n",
    "    - Number of samples: $M=100$.\n",
    "    - Training epochs:  $N=10$ (iterating over all of the samples 10 times).\n",
    "    - Denote `batch_size` with $m$.\n",
    "\n",
    "- Note: `batch_size` and `num_epochs` are **hyper-parameters** (chosen by the user prior to training)\n",
    "\n",
    "<details>\n",
    "  <summary>How many gradient updates in each algorithm?</summary>\n",
    "\n",
    "| Method|# Gradients Updates (=iterations) |\n",
    "|-------|----------------------------------|\n",
    "| **Batch** Gradient Descent, $m=100$| $\\frac{M}{m} \\cdot N = 1 * 10 = 10 $ |\n",
    "| **Stochastic** Gradient Descent, $m=1$ |  $\\frac{M}{m} \\cdot N =100 * 10 = 1000$ |\n",
    "| **Mini-Batch** Gradient Descent, $m=10$ | $\\frac{M}{m} \\cdot N =10 * 10 = 100$ |\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "  <summary>How this affects learning rate?</summary>\n",
    "\n",
    "* The update speed affects how we tune the **learning rate**, which is also a hyper-parameter.\n",
    "* Usually, `learning_rate` $\\propto$ `batch_size` (larger batches enable using higher learning rates).\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### GD Comparison Summary\n",
    "---\n",
    "\n",
    "| Method|Accuracy | Update Speed | Memory Usage |Online Learning |\n",
    "|---|---|---|---|---|\n",
    "| **Batch** Gradient Descent | Good | Slow |  High | No |\n",
    "| **Stochastic** Gradient Descent |  Good (with softening) | Fast |  Low |  Yes |\n",
    "| **Mini-Batch** Gradient Descent | Good | Medium | Medium | Yes (depends on the MB size) |\n",
    "\n",
    "\n",
    "* **\"Online\"** - samples arrive while the algorithm runs (that is, when the algorithm starts running, not all samples exist)\n",
    "* Note: all the Gradient Descent algorithms require **scaling** if the features are not within the same range!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- #### Update Speed\n",
    "---\n",
    "* Assume: number of samples $M=100$ and we wish to train for $N=10$ epochs (iterating over all of the samples 10 times).\n",
    "* Denote `batch_size` with $m$.\n",
    "* `batch_size` and `num_epochs` are **hyper-parameters**: parameters that are chosen by the user prior to the training stage, they are not learned.\n",
    "\n",
    "| Method|# Gradients Updates (=iterations) |\n",
    "|-------|----------------------------------|\n",
    "| **Batch** Gradient Descent, $m=100$| $\\frac{M}{m} \\cdot N = 1 * 10$ |\n",
    "| **Stochastic** Gradient Descent, $m=1$ |  $\\frac{M}{m} \\cdot N =100 * 10 = 1000$ |\n",
    "| **Mini-Batch** Gradient Descent, $m=10$ | $\\frac{M}{m} \\cdot N =10 * 10 = 100$ |\n",
    "\n",
    "* The update speed affects how we tune the **learning rate**, which is also a hyper-parameter (usually, larger batches enable using higher learning rates). -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Challenges\n",
    "---\n",
    "* Choosing a **learning rate**.\n",
    "    * Defining **learning schedule**.\n",
    "* Working with features of different scales (e.g. heights (cm), weights (kg) and age (scalar)).\n",
    "* Avoiding **local minima** (or *suboptimal* minima)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###  <img src=\"https://img.icons8.com/fluent/96/000000/sample-rate.png\" style=\"height:50px;display:inline\"> The Learning Rate\n",
    "---\n",
    "* **Learning Rate**: the size of step taken in each iteration.\n",
    "    * Too *small* $\\rightarrow$ the algorithm will have to go through many iterations to converge, which will take a long time.\n",
    "    * Too *high* $\\rightarrow$ might make the algorithm diverge as it may miss the minimum.\n",
    "<center><img src=\"./assets/lr.png\" style=\"height:250px\"></center>\n",
    "\n",
    "<a href=\"https://www.jeremyjordan.me/nn-learning-rate/\">Image Source</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"./assets/gd_converge_diverge.gif\" style=\"height:300px\"></center>\n",
    "\n",
    "<a href=\"https://towardsai.net/p/machine-learning/analysis-of-learning-rate-in-gradient-descent-algorithm-using-python\">Image Source</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/dusk/64/000000/classroom.png\" style=\"height:50px;display:inline\"> Example - (Multivariate) Linear Least Squares\n",
    "---\n",
    "* **Problem Formulation**\n",
    "    * $y \\in \\mathbb{R}^N$ - Vector of values.\n",
    "    * $X \\in \\mathbb{R}^{N \\times L}$ - Data matrix with $N$ examples and *$L$ features*.\n",
    "    * $w \\in \\mathbb{R}^L$ - The *parameters* to be learned, a **weight for each feature**.\n",
    "* **Goal**: find $w$ that best fits the measurement y, that is, find a *weighted linear combination* of the feature vector to best fit the measurment $y$.\n",
    "* Mathematiacally, the problem is:\n",
    "$$\\min_w f(w;x,y) = \\min_w \\sum_{i=1}^N||x_i w-y_i||^2 $$\n",
    "* In vector form:\n",
    "$$\\min_w f(w;x,y) = \\min_w ||Xw - Y||^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/dusk/64/000000/idea.png\" style=\"height:30px;display:inline\">   (Multivariate) LLS - Analytical Solution\n",
    "---\n",
    "* Mathematically:\n",
    "$$\\min_w f(w;x,y) = \\min_w ||Xw - Y||^2 = \\min_w (Xw-Y)^T(Xw-Y)= \\min_w (w^TX^TXw -2w^TX^TY + Y^TY)$$\n",
    "* The derivative: \n",
    "$$\\nabla_w f(w;x,y) = (X^TX + X^TX)w -2X^TY = 0$$\n",
    "$$ \\rightarrow w=(X^TX)^{-1}X^TY $$ $$X^TX \\in \\mathbb{R}^{L \\times L} $$\n",
    "\n",
    "* Notice how the gradient is dependent on the features, which is why scaling them is important when applying gradient descent (however, scaling is not necessary if we use the closed-form solution, the least-squares solution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>911366</td>\n",
       "      <td>B</td>\n",
       "      <td>11.620</td>\n",
       "      <td>18.18</td>\n",
       "      <td>76.38</td>\n",
       "      <td>408.8</td>\n",
       "      <td>0.11750</td>\n",
       "      <td>0.14830</td>\n",
       "      <td>0.102000</td>\n",
       "      <td>0.055640</td>\n",
       "      <td>...</td>\n",
       "      <td>25.40</td>\n",
       "      <td>88.14</td>\n",
       "      <td>528.1</td>\n",
       "      <td>0.1780</td>\n",
       "      <td>0.28780</td>\n",
       "      <td>0.31860</td>\n",
       "      <td>0.14160</td>\n",
       "      <td>0.2660</td>\n",
       "      <td>0.09270</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>868871</td>\n",
       "      <td>B</td>\n",
       "      <td>11.280</td>\n",
       "      <td>13.39</td>\n",
       "      <td>73.00</td>\n",
       "      <td>384.8</td>\n",
       "      <td>0.11640</td>\n",
       "      <td>0.11360</td>\n",
       "      <td>0.046350</td>\n",
       "      <td>0.047960</td>\n",
       "      <td>...</td>\n",
       "      <td>15.77</td>\n",
       "      <td>76.53</td>\n",
       "      <td>434.0</td>\n",
       "      <td>0.1367</td>\n",
       "      <td>0.18220</td>\n",
       "      <td>0.08669</td>\n",
       "      <td>0.08611</td>\n",
       "      <td>0.2102</td>\n",
       "      <td>0.06784</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>863031</td>\n",
       "      <td>B</td>\n",
       "      <td>11.640</td>\n",
       "      <td>18.33</td>\n",
       "      <td>75.17</td>\n",
       "      <td>412.5</td>\n",
       "      <td>0.11420</td>\n",
       "      <td>0.10170</td>\n",
       "      <td>0.070700</td>\n",
       "      <td>0.034850</td>\n",
       "      <td>...</td>\n",
       "      <td>29.26</td>\n",
       "      <td>85.51</td>\n",
       "      <td>521.7</td>\n",
       "      <td>0.1688</td>\n",
       "      <td>0.26600</td>\n",
       "      <td>0.28730</td>\n",
       "      <td>0.12180</td>\n",
       "      <td>0.2806</td>\n",
       "      <td>0.09097</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>893548</td>\n",
       "      <td>B</td>\n",
       "      <td>13.050</td>\n",
       "      <td>13.84</td>\n",
       "      <td>82.71</td>\n",
       "      <td>530.6</td>\n",
       "      <td>0.08352</td>\n",
       "      <td>0.03735</td>\n",
       "      <td>0.004559</td>\n",
       "      <td>0.008829</td>\n",
       "      <td>...</td>\n",
       "      <td>17.40</td>\n",
       "      <td>93.96</td>\n",
       "      <td>672.4</td>\n",
       "      <td>0.1016</td>\n",
       "      <td>0.05847</td>\n",
       "      <td>0.01824</td>\n",
       "      <td>0.03532</td>\n",
       "      <td>0.2107</td>\n",
       "      <td>0.06580</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>8912909</td>\n",
       "      <td>B</td>\n",
       "      <td>11.940</td>\n",
       "      <td>20.76</td>\n",
       "      <td>77.87</td>\n",
       "      <td>441.0</td>\n",
       "      <td>0.08605</td>\n",
       "      <td>0.10110</td>\n",
       "      <td>0.065740</td>\n",
       "      <td>0.037910</td>\n",
       "      <td>...</td>\n",
       "      <td>27.29</td>\n",
       "      <td>92.20</td>\n",
       "      <td>546.1</td>\n",
       "      <td>0.1116</td>\n",
       "      <td>0.28130</td>\n",
       "      <td>0.23650</td>\n",
       "      <td>0.11550</td>\n",
       "      <td>0.2465</td>\n",
       "      <td>0.09981</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>8611161</td>\n",
       "      <td>B</td>\n",
       "      <td>13.340</td>\n",
       "      <td>15.86</td>\n",
       "      <td>86.49</td>\n",
       "      <td>520.0</td>\n",
       "      <td>0.10780</td>\n",
       "      <td>0.15350</td>\n",
       "      <td>0.116900</td>\n",
       "      <td>0.069870</td>\n",
       "      <td>...</td>\n",
       "      <td>23.19</td>\n",
       "      <td>96.66</td>\n",
       "      <td>614.9</td>\n",
       "      <td>0.1536</td>\n",
       "      <td>0.47910</td>\n",
       "      <td>0.48580</td>\n",
       "      <td>0.17080</td>\n",
       "      <td>0.3527</td>\n",
       "      <td>0.10160</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>859487</td>\n",
       "      <td>B</td>\n",
       "      <td>12.780</td>\n",
       "      <td>16.49</td>\n",
       "      <td>81.37</td>\n",
       "      <td>502.5</td>\n",
       "      <td>0.09831</td>\n",
       "      <td>0.05234</td>\n",
       "      <td>0.036530</td>\n",
       "      <td>0.028640</td>\n",
       "      <td>...</td>\n",
       "      <td>19.76</td>\n",
       "      <td>85.67</td>\n",
       "      <td>554.9</td>\n",
       "      <td>0.1296</td>\n",
       "      <td>0.07061</td>\n",
       "      <td>0.10390</td>\n",
       "      <td>0.05882</td>\n",
       "      <td>0.2383</td>\n",
       "      <td>0.06410</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>893783</td>\n",
       "      <td>B</td>\n",
       "      <td>11.700</td>\n",
       "      <td>19.11</td>\n",
       "      <td>74.33</td>\n",
       "      <td>418.7</td>\n",
       "      <td>0.08814</td>\n",
       "      <td>0.05253</td>\n",
       "      <td>0.015830</td>\n",
       "      <td>0.011480</td>\n",
       "      <td>...</td>\n",
       "      <td>26.55</td>\n",
       "      <td>80.92</td>\n",
       "      <td>483.1</td>\n",
       "      <td>0.1223</td>\n",
       "      <td>0.10870</td>\n",
       "      <td>0.07915</td>\n",
       "      <td>0.05741</td>\n",
       "      <td>0.3487</td>\n",
       "      <td>0.06958</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>887181</td>\n",
       "      <td>M</td>\n",
       "      <td>15.660</td>\n",
       "      <td>23.20</td>\n",
       "      <td>110.20</td>\n",
       "      <td>773.5</td>\n",
       "      <td>0.11090</td>\n",
       "      <td>0.31140</td>\n",
       "      <td>0.317600</td>\n",
       "      <td>0.137700</td>\n",
       "      <td>...</td>\n",
       "      <td>31.64</td>\n",
       "      <td>143.70</td>\n",
       "      <td>1226.0</td>\n",
       "      <td>0.1504</td>\n",
       "      <td>0.51720</td>\n",
       "      <td>0.61810</td>\n",
       "      <td>0.24620</td>\n",
       "      <td>0.3277</td>\n",
       "      <td>0.10190</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>903483</td>\n",
       "      <td>B</td>\n",
       "      <td>8.734</td>\n",
       "      <td>16.84</td>\n",
       "      <td>55.27</td>\n",
       "      <td>234.3</td>\n",
       "      <td>0.10390</td>\n",
       "      <td>0.07428</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>22.80</td>\n",
       "      <td>64.01</td>\n",
       "      <td>317.0</td>\n",
       "      <td>0.1460</td>\n",
       "      <td>0.13100</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.2445</td>\n",
       "      <td>0.08865</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "469   911366         B       11.620         18.18           76.38      408.8   \n",
       "139   868871         B       11.280         13.39           73.00      384.8   \n",
       "106   863031         B       11.640         18.33           75.17      412.5   \n",
       "309   893548         B       13.050         13.84           82.71      530.6   \n",
       "286  8912909         B       11.940         20.76           77.87      441.0   \n",
       "81   8611161         B       13.340         15.86           86.49      520.0   \n",
       "69    859487         B       12.780         16.49           81.37      502.5   \n",
       "310   893783         B       11.700         19.11           74.33      418.7   \n",
       "258   887181         M       15.660         23.20          110.20      773.5   \n",
       "391   903483         B        8.734         16.84           55.27      234.3   \n",
       "\n",
       "     smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "469          0.11750           0.14830        0.102000             0.055640   \n",
       "139          0.11640           0.11360        0.046350             0.047960   \n",
       "106          0.11420           0.10170        0.070700             0.034850   \n",
       "309          0.08352           0.03735        0.004559             0.008829   \n",
       "286          0.08605           0.10110        0.065740             0.037910   \n",
       "81           0.10780           0.15350        0.116900             0.069870   \n",
       "69           0.09831           0.05234        0.036530             0.028640   \n",
       "310          0.08814           0.05253        0.015830             0.011480   \n",
       "258          0.11090           0.31140        0.317600             0.137700   \n",
       "391          0.10390           0.07428        0.000000             0.000000   \n",
       "\n",
       "     ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
       "469  ...          25.40            88.14       528.1            0.1780   \n",
       "139  ...          15.77            76.53       434.0            0.1367   \n",
       "106  ...          29.26            85.51       521.7            0.1688   \n",
       "309  ...          17.40            93.96       672.4            0.1016   \n",
       "286  ...          27.29            92.20       546.1            0.1116   \n",
       "81   ...          23.19            96.66       614.9            0.1536   \n",
       "69   ...          19.76            85.67       554.9            0.1296   \n",
       "310  ...          26.55            80.92       483.1            0.1223   \n",
       "258  ...          31.64           143.70      1226.0            0.1504   \n",
       "391  ...          22.80            64.01       317.0            0.1460   \n",
       "\n",
       "     compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "469            0.28780          0.31860               0.14160          0.2660   \n",
       "139            0.18220          0.08669               0.08611          0.2102   \n",
       "106            0.26600          0.28730               0.12180          0.2806   \n",
       "309            0.05847          0.01824               0.03532          0.2107   \n",
       "286            0.28130          0.23650               0.11550          0.2465   \n",
       "81             0.47910          0.48580               0.17080          0.3527   \n",
       "69             0.07061          0.10390               0.05882          0.2383   \n",
       "310            0.10870          0.07915               0.05741          0.3487   \n",
       "258            0.51720          0.61810               0.24620          0.3277   \n",
       "391            0.13100          0.00000               0.00000          0.2445   \n",
       "\n",
       "     fractal_dimension_worst  Unnamed: 32  \n",
       "469                  0.09270          NaN  \n",
       "139                  0.06784          NaN  \n",
       "106                  0.09097          NaN  \n",
       "309                  0.06580          NaN  \n",
       "286                  0.09981          NaN  \n",
       "81                   0.10160          NaN  \n",
       "69                   0.06410          NaN  \n",
       "310                  0.06958          NaN  \n",
       "258                  0.10190          NaN  \n",
       "391                  0.08865          NaN  \n",
       "\n",
       "[10 rows x 33 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's load the cancer dataset\n",
    "dataset = pd.read_csv('./datasets/cancer_dataset.csv')\n",
    "# print the number of rows in the data set\n",
    "number_of_rows = len(dataset)\n",
    "# reminder, the data looks like this\n",
    "dataset.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def plot_3d_lls(x, y, z, lls_sol, title=\"\"):\n",
    "    # plot\n",
    "    fig = plt.figure(figsize=(5, 5))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(x, y, z, label='Y')\n",
    "    ax.scatter(x, y, lls_sol, label='Xw')\n",
    "    ax.legend()\n",
    "    ax.set_xlabel('Radius Mean')\n",
    "    ax.set_ylabel('Area Mean')\n",
    "    ax.set_zlabel('Perimeter Mean')\n",
    "    ax.set_title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def batch_generator(x, y, batch_size, shuffle=True):\n",
    "    \"\"\"\n",
    "    This function generates batches for a given dataset x.\n",
    "    \"\"\"\n",
    "    N, L = x.shape\n",
    "    num_batches = N // batch_size\n",
    "    batch_x = []\n",
    "    batch_y = []\n",
    "    if shuffle:\n",
    "        # shuffle\n",
    "        rand_gen = np.random.RandomState(0)\n",
    "        shuffled_indices = rand_gen.permutation(np.arange(N))\n",
    "        x = x[shuffled_indices, :]\n",
    "        y = y[shuffled_indices, :]\n",
    "    for i in range(N):\n",
    "        batch_x.append(x[i, :])\n",
    "        batch_y.append(y[i, :])\n",
    "        if len(batch_x) == batch_size:\n",
    "            yield np.array(batch_x).reshape(batch_size, L), np.array(batch_y).reshape(batch_size, 1)\n",
    "            batch_x = []\n",
    "            batch_y = []\n",
    "    if batch_x:\n",
    "        yield np.array(batch_x).reshape(-1, L), np.array(batch_y).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* **Pseudocode** for Linear Regression:\n",
    "    * **Require**: Learning rate $\\alpha_k$\n",
    "    * **Require**: Initial parameter $w$\n",
    "    * **While** stopping criterion not met **do**\n",
    "        * Sample a minibatch of $m$ examples from the training set ($m=1$ for SGD)\n",
    "        * Set $\\tilde{X} = [x_1,...,x_m] $ with corresponding targets $\\tilde{Y} = [y_1,...,y_m]$\n",
    "        * Compute gradient: $g \\leftarrow 2\\tilde{X}^T\\tilde{X}w - 2\\tilde{X}^T \\tilde{Y}$\n",
    "        * Apply update: $w \\leftarrow w - \\alpha_k g$\n",
    "        * $k \\leftarrow k + 1$\n",
    "    * **end while**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# multivaraite mini-batch gradient descent\n",
    "X = dataset[['radius_mean', 'area_mean']].values\n",
    "Y = dataset[['perimeter_mean']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Scaling\n",
    "X = (X - X.mean(axis=0, keepdims=True)) / X.std(axis=0, keepdims=True)\n",
    "Y = (Y - Y.mean(axis=0, keepdims=True)) / Y.std(axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total batches: 56\n"
     ]
    }
   ],
   "source": [
    "N = X.shape[0]\n",
    "batch_size = 10\n",
    "num_batches = N // batch_size\n",
    "print(\"total batches:\", num_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0 batch: 0  w =  [0. 0.]\n",
      "iter: 0 batch: 50  w =  [0.4391737  0.41735388]\n",
      "iter: 1 batch: 0  w =  [0.45621431 0.43296123]\n",
      "iter: 1 batch: 50  w =  [0.50458588 0.46953377]\n",
      "iter: 2 batch: 0  w =  [0.50659246 0.46976394]\n",
      "iter: 2 batch: 50  w =  [0.51664408 0.46913792]\n",
      "iter: 3 batch: 0  w =  [0.51715596 0.46786206]\n",
      "iter: 3 batch: 50  w =  [0.52339741 0.46367398]\n",
      "iter: 4 batch: 0  w =  [0.52374047 0.4622501 ]\n",
      "iter: 4 batch: 50  w =  [0.5295512  0.45779193]\n",
      "iter: 5 batch: 0  w =  [0.52985556 0.456353  ]\n",
      "iter: 5 batch: 50  w =  [0.53556725 0.45194588]\n",
      "iter: 6 batch: 0  w =  [0.53584598 0.45050492]\n",
      "iter: 6 batch: 50  w =  [0.54149192 0.44617918]\n",
      "iter: 7 batch: 0  w =  [0.54174661 0.44473749]\n",
      "iter: 7 batch: 50  w =  [0.54733087 0.44049501]\n",
      "iter: 8 batch: 0  w =  [0.54756198 0.43905271]\n",
      "iter: 8 batch: 50  w =  [0.55308576 0.43489257]\n",
      "iter: 9 batch: 0  w =  [0.55329364 0.43344969]\n",
      "iter: 9 batch: 50  w =  [0.55875783 0.42937075]\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "alpha_k = 0.001\n",
    "batch_gen = batch_generator(X, Y, batch_size, shuffle=True)\n",
    "L = X.shape[-1]\n",
    "# initialize w\n",
    "w = np.zeros((L, 1))\n",
    "for i in range(num_epochs):\n",
    "    for batch_i, batch in enumerate(batch_gen):\n",
    "        batch_x, batch_y = batch\n",
    "        if batch_i % 50 == 0:\n",
    "            print(\"iter:\", i, \"batch:\", batch_i, \" w = \", w.flatten())\n",
    "        gradient = 2 * batch_x.T @ batch_x @ w - 2 * batch_x.T @ batch_y\n",
    "        w = w - alpha_k * gradient\n",
    "    batch_gen = batch_generator(X, Y, batch_size, shuffle=True)\n",
    "\n",
    "lls_sol = X @ w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w:  [0.55894282 0.42792729]\n"
     ]
    }
   ],
   "source": [
    "# plot\n",
    "plot_3d_lls(X[:,0], X[:, 1], Y, lls_sol, \"Breast Cancer - Radius Mean vs. Area Mean vs. Perimeter Mean - LLS Mini-Batch GD\")\n",
    "print(\"w: \", w.flatten())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<!-- ## <img src=\"https://img.icons8.com/clouds/100/000000/stopwatch.png\" style=\"height:50px;display:inline\"> Learning Rate Scheduling (Annealing)\n",
    "---\n",
    "* When training deep networks, it is usually helpful to anneal (gradually change the rate) the learning rate over time. \n",
    "    * **Physics intuition**: with a high learning rate, the system contains too much *kinetic energy* and the parameter vector bounces around chaotically, unable to settle down into deeper, but narrower parts of the loss function. \n",
    "* Knowing when to decay the learning rate can be tricky: decay it **slowly** and youâ€™ll be wasting computation bouncing around chaotically with little improvement for a long time. But decay it **too aggressively** and the system will cool too quickly, unable to reach the best position it can. \n",
    "* There are three common types of implementing the learning rate decay: step deacy, exponential decay and $1/t$ decay.\n",
    "    * Recently, *cyclic* learning schedulers, such as <a href=\"https://www.kaggle.com/residentmario/one-cycle-learning-rate-schedulers\">One-cycle learning rate scheduler</a> or *cosine* scheduling, have been gaining popularity as well. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<!-- * **(Multi) Step decay**: Reduce the learning rate by some factor every few epochs. \n",
    "    * Typical values might be reducing the learning rate by a half every 5 epochs, or by 0.1 every 20 epochs. These numbers depend heavily on the type of problem and the model. \n",
    "    * One heuristic you may see in practice is to watch the *validation error* while training with a fixed learning rate, and reduce the learning rate by a constant (e.g. 0.5) whenever the validation error stops improving. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<!-- * **Exponential decay**: has the mathematical form: $$\\alpha = \\alpha_0 \\exp(-kt),$$ where $\\alpha_0, k$ are hyperparameters and $t$ is the iteration number (but you can also use units of epochs).\n",
    "    * $\\alpha_0$ is the initial learning rate.\n",
    "    * $k$ is also referred to as the `gamma` ($\\gamma$) hyperparameter. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<!-- * **1/t decay**: has the mathematical form: $$\\alpha = \\frac{\\alpha_0}{1+kt},$$ where $\\alpha_0,k$ are hyperparameters and $t$ is the iteration number. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<!-- * **Cosine annealing**: has the mathematical form: $$ \\alpha = \\alpha_{min} +\\frac{1}{2}(\\alpha_0 - \\alpha_{min})\\left(1 +\\cos\\left(\\frac{t}{t_{max}}\\pi\\right) \\right), $$ where $\\alpha_{min}$ is the minimum learning rate (deafult is 0) and $t_{max}$ is number of iterations to perform a cycle. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<!-- * In practice, we usually find that the **step decay** is slightly preferable because the hyperparameters it involves (the fraction of decay and the step timings in units of epochs) are more interpretable than the hyperparameter $k$. \n",
    "* Lastly, if you can afford the computational budget, you can try a slower decay and train for a longer time. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <img src=\"https://img.icons8.com/clouds/100/000000/stopwatch.png\" style=\"height:50px;display:inline\"> Learning Rate Scheduling (Annealing)\n",
    "\n",
    "---\n",
    "<div style=\"width:100%;height:800px;line-height:3em;overflow:scroll;padding:5px;\">\n",
    "\n",
    "- **Purpose**: Gradually reduce the learning rate during training to improve convergence.\n",
    "\n",
    "- **Physics Intuition**: A high learning rate can cause the parameter vector to oscillate chaotically, preventing convergence to deeper, narrower minima.\n",
    "\n",
    "- **Balancing Act**:  \n",
    "  - **Slow decay** wastes computation by bouncing chaotically for too long.  \n",
    "  - **Fast decay** may \"freeze\" the system before reaching the best position.\n",
    "\n",
    "**Common Decay Methods**:\n",
    "  - **Step Decay**: Reduce the learning rate by a constant factor at fixed intervals (e.g., by half every 5 epochs).\n",
    "  - **Exponential Decay**: $$\\alpha = \\alpha_0 \\exp(-kt)  $$ \n",
    "  - **1/t Decay**: $$\\alpha = \\frac{\\alpha_0}{1+kt}\\,,$$ decaying inversely with time.\n",
    "    \n",
    "  - **Cosine Annealing**: Smooth periodic reduction with formula:  \n",
    "    $$\\alpha = \\alpha_{min} + \\frac{1}{2} (\\alpha_0 - \\alpha_{min}) \\left(1 + \\cos\\left(\\frac{t}{t_{max}} \\pi\\right)\\right)$$\n",
    "\n",
    "  Hyperparameters: $k$ - decay-rate, $\\alpha_0$ - initial learning rate.\n",
    "\n",
    "- **Practical Tips**:  \n",
    "    - Step decay is often preferred due to its interpretable hyperparameters (decay fraction and timing). \n",
    "    - Slower decay combined with longer training may yield better results.\n",
    "    - **Emerging Schedulers**: Cyclic learning rates, like the [One-Cycle Learning Rate Scheduler](https://www.kaggle.com/residentmario/one-cycle-learning-rate-schedulers), are increasingly popular.\n",
    "\n",
    "    - **Learning Rate Warmup**:  Gradually increase the learning rate from a small value during initial epochs to stabilize early training.\n",
    "\n",
    "<center><img src=\"./assets/Comparison-between-constant-lr-scheduler-and-cosine-annealing-lr-scheduler-with-linear.png\" style=\"height:200px\"></center>\n",
    "\n",
    "    \n",
    "<a href=\"https://www.researchgate.net/figure/Comparison-between-constant-lr-scheduler-and-cosine-annealing-lr-scheduler-with-linear_fig1_336936339\">Image Source</a>\n",
    "\n",
    "  </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/cotton/64/000000/olympic-torch.png\" style=\"height:50px;display:inline\"> Learning Rate Scheduling in PyTorch\n",
    "---\n",
    "* We will use learning rate scheduling to train the neural network models later in the course.\n",
    "* PyTorch offers several schedulers which can be found <a href=\"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\">here</a>.\n",
    "* A typical workflow with schedulers (learning rate scheduling should be applied **after** optimizerâ€™s update):\n",
    "<code>\n",
    "scheduler = ...\n",
    "for epoch in range(100):\n",
    "    train(...)\n",
    "    validate(...)\n",
    "    scheduler.step()\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* <a href=\"https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html#torch.optim.lr_scheduler.StepLR\">`torch.optim.lr_scheduler.StepLR`</a>\n",
    "* <a href=\"https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.MultiStepLR.html#torch.optim.lr_scheduler.MultiStepLR\">`torch.optim.lr_scheduler.MultiStepLR`</a>\n",
    "* <a href=\"https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ExponentialLR.html#torch.optim.lr_scheduler.ExponentialLR\">`torch.optim.lr_scheduler.ExponentialLR`</a>\n",
    "* <a href=\"https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html#torch.optim.lr_scheduler.CosineAnnealingLR\">`torch.optim.lr_scheduler.CosineAnnealingLR`</a>\n",
    "* <a href=\"https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.OneCycleLR.html#torch.optim.lr_scheduler.OneCycleLR\">`torch.optim.lr_scheduler.OneCycleLR`</a>\n",
    "* <a href=\"https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CyclicLR.html#torch.optim.lr_scheduler.CyclicLR\">`torch.optim.lr_scheduler.CyclicLR`</a>\n",
    "* And more..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Reducing LR on Plateau\n",
    "---\n",
    "* Reduce learning rate when <ins>a metric has stopped improving</ins> (usually the validation accuracy).\n",
    "* Models often benefit from reducing the learning rate by a factor of 2-10 once learning does not improve. \n",
    "* This scheduler reads a metrics quantity and if no improvement is seen for a `patience` number of epochs, the learning rate is reduced.\n",
    "* In PyTorch: <a href=\"https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html#torch.optim.lr_scheduler.ReduceLROnPlateau\">`torch.optim.lr_scheduler.ReduceLROnPlateau`</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"./assets/tut_opt_reduce_lr.png\" style=\"height:300px\"></center>\n",
    "\n",
    "* <a href=\"https://www.researchgate.net/publication/339252130_Exponential_Step_Sizes_for_Non-Convex_Optimization\">Exponential Step Sizes for Non-Convex Optimization, Li et al. 2020</a>.\n",
    "* Plots of the train loss and test accuracy for training a 20-layer Residual Network to do image classification on CIFAR-10.\n",
    "* The number of milestones in the legend denotes how many times we can choose to decrease the step size during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/cotton/64/000000/stop-sign.png\" style=\"height:50px;display:inline\"> Question time!\n",
    "---\n",
    "#### Lecture question\n",
    "This question relates to slide ~26 in the Optimization lecture slides.\n",
    "\n",
    "For multivariate quadratic optimization via GD i.e.\n",
    "$$f(w)=\\frac{1}{2}w^THw$$\n",
    "$$\\nabla f(w)=Hw$$\n",
    "$$w(t)=w(t-1)-\\eta Hw(t-1)$$\n",
    "We saw in the lecture (via Eigenvalue Decoposition) that \n",
    "$$H=U^T \\Lambda U\\quad ; \\quad z(t)\\triangleq U^T w(t)$$\n",
    "where $\\Lambda$ is the eigenvalue matrix with eigenvalues $\\lambda_1 \\leq \\lambda_2 \\leq...\\leq \\lambda_d$.\n",
    "$$z(t)=(I-\\eta\\Lambda)^t z(0)$$\n",
    "\n",
    "Show that:\n",
    "1. $$f(w(t))=\\sum_{i=1}^d (1-\\eta\\lambda_i)^{2t}\\lambda_iz^2_i(0)$$\n",
    "2. $$\\mathrm{rate}(\\eta)=\\max(|1-\\eta\\lambda_{\\min}|,|1-\\eta\\lambda_{\\max}|)$$ (explain informally)\n",
    "3. $$\\eta_{\\mathrm{opt}}=\\arg\\min_{\\eta}\\mathrm{rate}(\\eta)=\\frac{2}{\\lambda_{\\max}+\\lambda_{\\min}}$$\n",
    "4. $$R_{\\mathrm{optimal}} = \\min_{\\eta}\\mathrm{rate}(\\eta) = \\frac{\\lambda_{max}/ \\lambda_{min} - 1}{\\lambda_{max} / \\lambda_{min} + 1} = \\kappa \\text{(condition number)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/cotton/64/000000/checkmark.png\" style=\"height:50px;display:inline\"> Answer time!\n",
    "---\n",
    "<details>\n",
    "  <summary>Hint</summary>\n",
    "    \n",
    "    1. Use the fact that $U$ is orthogonal, i.e. $U^T U=I$.\n",
    "    \n",
    "    2. Consider what element dominates a decaying exponential sum.\n",
    "    \n",
    "    3. What is the derivative of $\\mathrm{rate}(\\eta)$? what happens at the non-differntiable critical point?\n",
    "    \n",
    "    4. Use section 3\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "    \n",
    "#### Section 0\n",
    "<span style=\"color:gray\">\n",
    "Show $z(t)=(I-\\eta\\Lambda)^t z(0)$ (Lecture)\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "We begin with the definition of the GD step: \n",
    "$$\n",
    "\\begin{aligned}\n",
    "w(t) &= w(t-1) -\\eta H w(t-1) \\\\\n",
    "&= (I -\\eta H)w(t-1) \\\\\n",
    "&= (I -\\eta U\\Lambda U^T)w(t-1)\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We now multiply by $U^T$ from the left-hand side and recall that we defined $$z(0) = U^Tw(0)\\,,$$ $$z(t)=U^Tw(t)$$ to get:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "U^Tw(t) &= U^T(UU^T - \\eta U\\Lambda U^T)w(t-1) \\\\\n",
    "&= (U^TU - \\eta U^TU\\Lambda) U^Tw(t-1) \\\\\n",
    "&= (I -\\eta \\Lambda)z(t-1), \n",
    "\\end{aligned}\n",
    "$$ \n",
    "where we used $UU^T=U^TU=I$.\n",
    "\n",
    "Next, we can unroll the GD steps up to step $t$ to get: $$  z(t) = (I -\\eta \\Lambda)^t z(0). $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Section 1\n",
    "<span style=\"color:gray\">\n",
    "Show that $$f(w(t))=\\sum_{i=1}^d (1-\\eta\\lambda_i)^{2t}\\lambda_iz^2_i(0)$$\n",
    "</span>\n",
    "<!-- Going back to the form with $w(t)$ by multiplying by $U$: $$ UU^T w(t) = U(I -\\eta \\Lambda)^t U^T w(0) $$\n",
    "<details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Using \n",
    "$$  z(t) = (I -\\eta \\Lambda)^t z(0) \\,,$$\n",
    "$$  z(t)\\triangleq U^T w(t) \\rightarrow w(t) = Uz(t)$$\n",
    "\n",
    "We get \n",
    "$$ \\begin{aligned}\n",
    "w(t) &= Uz(t) \\\\\n",
    "&= U (I -\\eta \\Lambda)^t z(0) \\\\\n",
    "&= U (I -\\eta \\Lambda)^t U w(0) \\\\\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Finally, let's plug-in this result in $f(w)$ to get: \n",
    "$$f(w(t))=\\frac{1}{2}w(t)^THw(t)$$\n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "f(w(t)) &= \\frac{1}{2}\\left[U (I -\\eta \\Lambda)^t z(0) \\right]^T U\\Lambda U^T\\left[U (I -\\eta \\Lambda)^t z(0) \\right] \\\\\n",
    "&= \\frac{1}{2}z(0)^T (I -\\eta \\Lambda)^t U^TU \\Lambda  U^TU (I -\\eta \\Lambda)^t z(0) \\\\\n",
    "&= \\frac{1}{2}z(0)^T (I -\\eta \\Lambda)^t \\Lambda (I -\\eta \\Lambda)^t z(0) \\\\\n",
    "&= \\frac{1}{2}z(0)^T (I -\\eta \\Lambda)^{2t}\\Lambda z(0) \\\\\n",
    "&= \\frac{1}{2}\\sum_{i=1}^d (1 - \\eta \\lambda_i)^{2t}\\lambda_i z_i(0)^2 \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Section 2\n",
    "<span style=\"color:gray\">\n",
    "Explain informally why the conergence rate is $$\\mathrm{rate}(\\eta)=\\max(|1-\\eta\\lambda_{\\min}|,|1-\\eta\\lambda_{\\max}|)$$\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<!-- Looking at the sum above, for $t>1$ this is a decaying exponential sum that is dominated by the slowest decaying components: -->\n",
    "- The expression we got in section 1 is a sum of exponentially decaying components\n",
    "$$(1 - \\eta \\lambda_i)^{2t}\\lambda_i z_i(0)^2$$\n",
    "- The sum is dominated by the <ins>slowest decaying component</ins>\n",
    "$$ \\begin{aligned}\n",
    "rate(\\eta) &= \\max_i |1-\\eta\\lambda_i| \\\\\n",
    "&=  max(|1-\\eta \\lambda_{max}|,|1-\\eta \\lambda_{min}|),\n",
    "\\end{aligned}$$\n",
    "where we denoted $\\lambda_1 =\\lambda_{min}, \\lambda_d = \\lambda_{max}. $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<details>\n",
    "    <summary>Why?</summary>\n",
    "    \n",
    "* If $1-\\eta \\lambda_{i} \\geq 0 $ then\n",
    "  $$\n",
    "  |1-\\eta \\lambda_{i}| = 1-\\eta \\lambda_{i} \\leq 1-\\eta \\lambda_{min} = |1-\\eta \\lambda_{min}|\n",
    "  $$\n",
    "  (We assume $\\forall i \\,, \\lambda_i \\geq 0$)\n",
    "\n",
    "* If $1-\\eta \\lambda_{i} < 0 $ then\n",
    "  $$\n",
    "  |1-\\eta \\lambda_{i}| = \\eta \\lambda_{i} - 1 \\leq \\eta \\lambda_{max} - 1 = |1-\\eta \\lambda_{max}|\n",
    "  $$\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Section 3\n",
    "<span style=\"color:gray\">\n",
    "Show the optimal learning rate is $$\\eta_{\\mathrm{opt}}=\\arg\\min_{\\eta}\\mathrm{rate}(\\eta)=\\frac{2}{\\lambda_{\\max}+\\lambda_{\\min}}$$\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Finding the optimal learning rate $\\eta_{optimal}$: \n",
    "- If we take the derivative of $rate(\\eta)$ we will get a constant at any differentiable point. \n",
    "- Thus, the minimum is reached in the non-differentiable critical point \n",
    "- This can also be seen geometrically by drawing the $\\max (\\cdot, \\cdot)$ function\n",
    "$$ |1-\\eta \\lambda_{max}| = |1-\\eta \\lambda_{min}| $$ $$ \\to \\eta \\lambda_{max} - 1 = 1-\\eta \\lambda_{min} $$ $$ \\eta_{optimal} = \\frac{2}{\\lambda_{min} + \\lambda_{max}}. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Section 4\n",
    "<span style=\"color:gray\">\n",
    "Show the optimal convergence rate is\n",
    "$$R_{\\mathrm{optimal}} = \\min_{\\eta}\\mathrm{rate}(\\eta) = \\frac{\\lambda_{max}/ \\lambda_{min} - 1}{\\lambda_{max} / \\lambda_{min} + 1} = \\kappa \\text{(condition number)} $$\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Finally, we plug $\\eta_{optimal}$ in the $rate(\\eta)$ to get:\n",
    "$$ R_{optimal} = rate(\\eta_{optimal}) = 1 -  \\frac{2}{\\lambda_{min} + \\lambda_{max}}\\cdot \\lambda_{min} $$\n",
    "$$ =\\frac{\\lambda_{max} - \\lambda_{min}}{\\lambda_{max} + \\lambda_{min}} = \\frac{ \\frac{\\lambda_{max}}{\\lambda_{min}} - 1 }{\\frac{\\lambda_{max}}{\\lambda_{min}} + 1 }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Extra\n",
    "For most step-sizes, the eigenvectors with largest eigenvalues converge the fastest. \n",
    "This triggers an explosion of progress in the first few iterations, before things slow down as the smaller eigenvectorsâ€™ struggles are revealed.\n",
    "\n",
    "In order to converge, each $|1-\\eta \\lambda_i|$ must be strictly less than 1. \n",
    "All workable step-sizes, therefore, fall in the interval: $$ 0 \\leq \\eta \\lambda_i \\leq 2 $$ The overall convergence rate is determined by the slowest error component, which must be either $\\lambda_{min}$ or $\\lambda_{max}$.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ### <img src=\"https://img.icons8.com/cotton/64/000000/checkmark.png\" style=\"height:50px;display:inline\"> Answer time!\n",
    "---\n",
    "<details>\n",
    "  <summary>Hint</summary>\n",
    "    \n",
    "    1. Use the fact that $U$ is orthogonal, i.e. $U^T U=I$.\n",
    "    \n",
    "    2. Consider what element dominates a decaying exponential sum.\n",
    "    \n",
    "    3. What is the derivative of $\\mathrm{rate}(\\eta)$? what happens at the non-differntiable critical point?\n",
    "    \n",
    "    4. Use section 3\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "  <summary>Solution</summary>\n",
    "    \n",
    "#### Section 1\n",
    "We begin with the definition of the GD step: $$ w(t) = w(t-1) -\\eta H w(t-1) = (I -\\eta H)w(t-1) = (I -\\eta U\\Lambda U^T)w(t-1). $$\n",
    "We now multiply by $U^T$ from the left-hand side and recall that we defined $z(0) = U^Tw(0), z(t)=U^Tw(t)$ to get:\n",
    "$$U^Tw(t) = U^T(UU^T - \\eta U\\Lambda U^T)w(t-1) \\to z(t) = (I -\\eta \\Lambda)z(t-1), $$ where we used $UU^T=U^TU=I$.\n",
    "\n",
    "Next, we can unroll the GD steps up to step $t$ to get: $$  z(t) = (I -\\eta \\Lambda)^t z(0). $$\n",
    "\n",
    "Going back to the form with $w(t)$ by multiplying by $U$: $$ UU^T w(t) = U(I -\\eta \\Lambda)^t U^T w(0) $$\n",
    "$$ \\to w(t) = U(I - \\eta \\Lambda)^t U^T w(0) $$\n",
    "\n",
    "Finally, let's plug-in this result in $f(w)$ to get: \n",
    "$$ f(w) = \\frac{1}{2}\\left[U(I -\\eta \\Lambda)^t U^Tw(0) \\right]^T U\\Lambda U^T\\left[U(I -\\eta \\Lambda)^t U^Tw(0) \\right] $$\n",
    "\n",
    "$$ = \\frac{1}{2}z(0)^T (I -\\eta \\Lambda)^t \\Lambda (I -\\eta \\Lambda)^t z(0) $$\n",
    "\n",
    "$$ = \\frac{1}{2}z(0)^T (I -\\eta \\Lambda)^{2t}\\Lambda z(0) $$\n",
    "\n",
    "$$ = \\frac{1}{2}\\sum_{i=1}^d (1 - \\eta \\lambda_i)^{2t}\\lambda_i z_i(0)^2$$\n",
    "\n",
    "#### Section 2\n",
    "Looking at the sum above, for $t>1$ this is a decaying exponential sum that is dominated by by the slowest decaying components: $$ rate(\\eta)=\\max_i |1-\\eta\\lambda_i| =  max(|1-\\eta \\lambda_{max}|,|1-\\eta \\lambda_{min}|),$$ where we denoted $\\lambda_1 =\\lambda_{min}, \\lambda_d = \\lambda_{max}. $\n",
    "#### Section 3\n",
    "Finding the optimal learning rate $\\eta_{optimal}$: if we take the derivative of $rate(\\eta)$ we will get a constant at any differentiable point. Thus, the minimum is reached in the non-differentiable critical point (you can also see this geometrically if you draw the $\\max (\\cdot, \\cdot)$ function): $$ |1-\\eta \\lambda_{max}| = |1-\\eta \\lambda_{min}| $$ $$ \\to \\eta \\lambda_{max} - 1 = 1-\\eta \\lambda_{min} $$ $$ \\eta_{optimal} = \\frac{2}{\\lambda_{min} + \\lambda_{max}}. $$\n",
    "#### Section 4\n",
    "Finally, we plug $\\eta_{optimal}$ in the $rate(\\eta)$ to get:\n",
    "$$ R_{optimal} = rate(\\eta_{optimal}) = 1 -  \\frac{2}{\\lambda_{min} + \\lambda_{max}}\\cdot \\lambda_{min} $$\n",
    "$$ =\\frac{\\lambda_{max} - \\lambda_{min}}{\\lambda_{max} + \\lambda_{min}} = \\frac{ \\frac{\\lambda_{max}}{\\lambda_{min}} - 1 }{\\frac{\\lambda_{max}}{\\lambda_{min}} + 1 }$$\n",
    "#### Extra\n",
    "For most step-sizes, the eigenvectors with largest eigenvalues converge the fastest. \n",
    "This triggers an explosion of progress in the first few iterations, before things slow down as the smaller eigenvectorsâ€™ struggles are revealed.\n",
    "\n",
    "In order to converge, each $|1-\\eta \\lambda_i|$ must be strictly less than 1. \n",
    "All workable step-sizes, therefore, fall in the interval: $$ 0 \\leq \\eta \\lambda_i \\leq 2 $$ The overall convergence rate is determined by the slowest error component, which must be either $\\lambda_{min}$ or $\\lambda_{max}$.\n",
    "</details> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/cotton/64/000000/stop-sign.png\" style=\"height:50px;display:inline\"> Question time!\n",
    "---\n",
    "#### Lecture question\n",
    "This question relates to slide ~32 in the Optimization lecture slides.\n",
    "$$f(w)=\\frac{1}{2}\\sum_{n=1}^N h_n w^2$$\n",
    "SGD update ($n(t)\\sim U(1,N)$)\n",
    "$$w(t)=w(t-1)-\\eta h_{n(t)}w(t-1)=(1-\\eta h_{n(t)})w(t-1)$$\n",
    "Define \n",
    "$$h\\triangleq \\mathbb{E}h_{n(t)}=\\frac{1}{N}\\sum_{n=1}^N h_n, \\quad \\rho\\triangleq \\mathrm{Var}(h_{n(t)})=\\frac{1}{N}\\sum_{n=1}^N h_n^2-h^2$$\n",
    "\n",
    "Show that \n",
    "1. $$\\mathbb{E}w(t)=(1-\\eta h)\\mathbb{E}w(t-1)$$\n",
    "2. $$\\mathbb{E}w^2(t)=\\left ((1-\\eta h)^2+\\eta^2\\rho\\right )\\mathbb{E}w^2(t-1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <img src=\"https://img.icons8.com/cotton/64/000000/checkmark.png\" style=\"height:50px;display:inline\"> Answer time!\n",
    "---\n",
    "<details>\n",
    "  <summary>Hint</summary>\n",
    "    \n",
    "1. Since $n(t)$ is uniform, $h_{n(t)}$ is independent of $w(t)$\n",
    "    \n",
    "2. Note that $\\mathbb{E}h_{n(t)}^2=\\rho+h^2$\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Section 1\n",
    "<span style=\"color:gray\">\n",
    "Show that\n",
    "$$\\mathbb{E}w(t)=(1-\\eta h)\\mathbb{E}w(t-1)$$\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "We get\n",
    "$$\\mathbb{E}w(t)=\\mathbb{E}[(1-\\eta h_{n(t)})w(t-1)]$$\n",
    "\n",
    "Since $n(t)$ is uniform, $h_{n(t)}$ is independent of $w(t)$, so \n",
    "$$\\mathbb{E}[h_{n(t)}w(t-1)]=\\mathbb{E}h_{n(t)}\\mathbb{E}w(t-1)$$\n",
    "\n",
    "We get \n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbb{E}w(t) &= \\mathbb{E}[(1-\\eta h_{n(t)})]\\mathbb{E}w(t-1)\\\\\n",
    "&= ((1-\\eta \\mathbb{E}h_{n(t)})\\mathbb{E}w(t-1) \\\\\n",
    "&= ((1-\\eta h)\\mathbb{E}w(t-1)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Section 2\n",
    "<span style=\"color:gray\">\n",
    "Show that\n",
    "$$\\mathbb{E}w^2(t)=\\left ((1-\\eta h)^2+\\eta^2\\rho\\right )\\mathbb{E}w^2(t-1)$$\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "$$\\begin{aligned}\n",
    "\\mathbb{E}w^2(t) &= \\mathbb{E}[(1-\\eta h_{n(t)})^2 w^2(t-1)] \\\\\n",
    "&=\\mathbb{E}[(1-2\\eta h_{n(t)}+\\eta^2 h_{n(t)}^2) w^2(t-1)] \n",
    "\\end{aligned}$$\n",
    "\n",
    "Since $n(t)$ is uniform, $h_{n(t)}$ is independent of $w(t)$, so \n",
    "$$\\mathbb{E}[h_{n(t)}w(t-1)]=\\mathbb{E}h_{n(t)}\\mathbb{E}w(t-1)$$\n",
    "\n",
    "We get \n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbb{E}w(t) &= \\mathbb{E}[1-2\\eta h_{n(t)}+\\eta^2 h_{n(t)}^2] \\mathbb{E}w^2(t-1) \\\\\n",
    "&= [1 - 2\\eta h + \\eta^2 \\mathbb{E} [h_{n(t)}^2] ] \\mathbb{E}w^2(t-1) \\\\\n",
    "&=[1-2\\eta h+\\eta^2 (\\rho+h^2)] \\mathbb{E}w^2(t-1)\n",
    "\\end{aligned}$$\n",
    "\n",
    "In the last transition we used\n",
    "$$ \\rho = \\frac{1}{N}\\sum_{n=1}^N h_n^2-h^2 \\rightarrow \\mathbb{E} [h_{n(t)}^2] = \\rho + h^2 $$\n",
    "\n",
    "Then we get \n",
    "\n",
    "$$\\begin{aligned} \n",
    "\\mathbb{E}w(t) &= [1-2\\eta h+\\eta^2 h^2+ \\eta^2\\rho] \\mathbb{E}w^2(t-1) \\\\ \n",
    "&= [(1-\\eta h)^2+ \\eta^2\\rho] \\mathbb{E}w^2(t-1) \n",
    "\\end{aligned}$$\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<!-- ## <img src=\"https://img.icons8.com/clouds/100/000000/hill-descent-control.png\" style=\"height:50px;display:inline\"> Momentum & Nesterov Momentum\n",
    "\n",
    "- **Challenge with GD**:  \n",
    "  - Standard gradient descent is simple but slow; small step-sizes ensure steady progress yet limit speed.\n",
    "\n",
    "- **Momentum Advantage**:  \n",
    "  - Introduces a \"short-term memory\" that accelerates convergence by building velocity in consistently descending directions.\n",
    "  - Can be viewed as a particle rolling on hilly terrain, where the gradient acts as force driving acceleration.\n",
    "\n",
    "- **Momentum Update Equations**:  \n",
    "  - $$ z^{k+1} = \\beta z^k - \\alpha\\nabla f(w^k) $$  \n",
    "  - $$ w^{k+1} = w^k + z^{k+1} $$  \n",
    "  - *(Here, $\\alpha$ is the learning rate and $\\beta$ (e.g., 0.99 or 0.999) controls momentum.)*\n",
    "\n",
    "- **Nesterov Momentum**:  \n",
    "  - A variant that uses a lookahead approach to further improve convergence rates.\n",
    "\n",
    "- **Key Insight**:  \n",
    "  - Momentum dampens oscillations, reducing kinetic energy and enabling faster progress towards the optimumâ€”especially effective in large batches.\n",
    "\n",
    "- [Momentum Demo](https://distill.pub/2017/momentum/)\n",
    " -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <img src=\"https://img.icons8.com/clouds/100/000000/hill-descent-control.png\" style=\"height:50px;display:inline\"> Momentum & Nesterov Momentum\n",
    "---\n",
    "* Gradient descent is simple and has many virtues, but **speed** is not one of them.\n",
    "* For a step-size small enough, gradient descent makes a monotonic improvement at every iteration. It should always converge (sometimes to a local minimum).\n",
    "* **Momentum update** is another optimization approach that *almost always* enjoys better convergence rates in deep networks. \n",
    "    * It can be seen as a **\"global\" (equally for all parameters) adaptive learning rate**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* This update can be motivated from a physical perspective of the optimization problem. In particular, the loss can be interpreted as the height of a *hilly terrain*.\n",
    "    *  Initializing the parameters with random numbers is equivalent to setting a particle with zero initial velocity at some location. The optimization process can then be seen as equivalent to the process of simulating the parameter vector (i.e. a particle) as rolling on the landscape.\n",
    "    * Since the force on the particle is related to the gradient of potential energy (i.e. $F=âˆ’\\nabla U$ ), the force felt by the particle is precisely the (negative) gradient of the loss function. \n",
    "    * Moreover, $F=ma$ so the (negative) gradient is in this view proportional to the acceleration of the particle.\n",
    "    * The physics view suggests an update in which the gradient only directly influences the velocity (and maintains information about the acceleration), which in turn has an effect on the position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient Descent with Momentum**:\n",
    "$$ z^{k+1} = \\beta z^k -\\alpha\\nabla f(w^k) $$ $$ w^{k+1} = w^k + z^{k+1} $$\n",
    "\n",
    "- $\\alpha$: Learning rate\n",
    "- $ z^k $: Memory term\n",
    "- $\\beta$: *Momentum* factor\n",
    "    - Must use  $0 \\leq \\beta < 1$ (why?)\n",
    "    - When $\\beta = 0$ , we recover gradient descent. \n",
    "    - Usually $\\beta \\sim 0.99$ (sometimes 0.999, if things are really bad), this appears to be the boost we need. Our iterations regain that speed and boldness it lost, speeding to the optimum with a renewed energy.\n",
    "* Effectively, $\\beta$ **damps the velocity** and reduces the kinetic energy of the system\n",
    "* Otherwise the particle would never come to a stop at the bottom of a hill.\n",
    "* With Momentum update, the parameter vector will build up velocity in any direction that has <ins>consistent gradient</ins>.\n",
    "* <a href=\"https://distill.pub/2017/momentum/\">Momentum Demo</a>\n",
    "* Note: Momentum usually works in **larger batches** and may break in smaller batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<!-- * Momentum proposes the following tweak to gradient descent, giving gradient descent **a short-term memory**: $$ z^{k+1} = \\beta z^k -\\alpha\\nabla f(w^k) $$ $$ w^{k+1} = w^k + z^{k+1} $$\n",
    "    * $\\alpha$ is the learning rate.\n",
    "\n",
    "* When $\\beta = 0$ , we recover gradient descent. But for $\\beta = 0.99$ (sometimes 0.999, if things are really bad), this appears to be the boost we need. Our iterations regain that speed and boldness it lost, speeding to the optimum with a renewed energy.\n",
    "* $\\beta$ is a variable that is sometimes called *momentum*.\n",
    "* Effectively, this variable **damps the velocity and reduces the kinetic energy of the system**, or otherwise the particle would never come to a stop at the bottom of a hill.\n",
    "* With Momentum update, the parameter vector will build up velocity in any direction that has <ins>consistent gradient</ins>.\n",
    "* <a href=\"https://distill.pub/2017/momentum/\">Momentum Demo</a>\n",
    "* Note: Momentum usually works in **larger batches** and may break in smaller batches. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"./assets/sgd-mom.png\" style=\"height:250px\"></center>\n",
    "\n",
    "* <a href=\"https://dominikschmidt.xyz/nesterov-momentum\">Image Source</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nesterov Momentum\n",
    "---\n",
    "- Nesterov Momentum is a slightly different version of the momentum update that has gained popularity. \n",
    "    - Stronger theoretical convergence guarantees for **convex functions** \n",
    "    - Works slightly better in practice.\n",
    "    \n",
    "    \n",
    "- Main idea: \n",
    "    - When the current weights at some position $w$, the momentum term is about to nudge the current gradient by $\\beta * z_k$. \n",
    "    - When we about to compute the gradient, use a <ins>lookahead</ins> $w + \\beta * z_k$ \n",
    "\n",
    "<!-- * Hence, it makes sense to compute the **gradient** at $x + \\beta * z_k$ instead of at the â€œold/staleâ€ position $x$, since while the gradient term always points in the right direction, the momentum term may not. \n",
    " -->\n",
    "- If the momentum term points in the wrong direction or overshoots, the gradient can still \"go back\" and correct it in the same update step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<!-- #### Nesterov Momentum\n",
    "---\n",
    "* Nesterov Momentum is a slightly different version of the momentum update that has gained popularity. \n",
    "* It enjoys stronger theoretical convergence guarantees for **convex functions** and in practice it also consistenly works slightly better than standard momentum.\n",
    "* The core idea behind Nesterov momentum is that when the current parameter vector is at some position $x$, then looking at the momentum update above, we know that the momentum term alone (i.e. ignoring the second term with the gradient) is about to nudge the parameter vector by $\\beta * z_k$. \n",
    "* Therefore, if we are about to compute the gradient, we can treat the future approximate position $x + \\beta * z_k$ as a **â€œlookaheadâ€** - this is a point in the vicinity of where we are soon going to end up. \n",
    "* Hence, it makes sense to compute the **gradient** at $x + \\beta * z_k$ instead of at the â€œold/staleâ€ position $x$, since while the gradient term always points in the right direction, the momentum term may not. \n",
    "* If the momentum term points in the wrong direction or overshoots, the gradient can still \"go back\" and correct it in the same update step. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* **Nesterov Momentum**: $$ z^{k+1} = \\beta z^k -\\alpha \\nabla f(w^k +\\beta z^k) $$ $$ w^{k+1} = w^k + z^{k+1} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"./assets/sgd-nag.png\" style=\"height:250px\"></center>\n",
    "\n",
    "* <a href=\"https://dominikschmidt.xyz/nesterov-momentum\">Image Source</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/cotton/64/000000/olympic-torch.png\" style=\"height:50px;display:inline\"> Momentum in PyTorch\n",
    "---\n",
    "* `torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True)`\n",
    "* (Note `momentum` $= \\beta$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hmichaeli/anaconda3/envs/deep_learn/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 completed\n",
      "Loss: 0.11388081312179565. Accuracy: 95.62\n",
      "--------------------\n",
      "Epoch 1 completed\n",
      "Loss: 0.014179451391100883. Accuracy: 96.16\n",
      "--------------------\n",
      "Epoch 2 completed\n",
      "Loss: 0.06918156892061234. Accuracy: 96.87\n",
      "--------------------\n",
      "Epoch 3 completed\n",
      "Loss: 0.054913464933633804. Accuracy: 97.48\n",
      "--------------------\n",
      "Epoch 4 completed\n",
      "Loss: 0.034778088331222534. Accuracy: 97.23\n",
      "--------------------\n",
      "Epoch 5 completed\n",
      "Loss: 0.011099637486040592. Accuracy: 97.88\n",
      "--------------------\n",
      "Epoch 6 completed\n",
      "Loss: 0.015837712213397026. Accuracy: 97.96\n",
      "--------------------\n",
      "Epoch 7 completed\n",
      "Loss: 0.008104035630822182. Accuracy: 98.0\n",
      "--------------------\n",
      "Epoch 8 completed\n",
      "Loss: 0.013296498917043209. Accuracy: 98.01\n",
      "--------------------\n",
      "Epoch 9 completed\n",
      "Loss: 0.021013470366597176. Accuracy: 98.02\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "# simple optimizer and lr scheduling example\n",
    "# courtesy of: deeplearningwizard.com/deep_learning/boosting_models_pytorch/lr_scheduling/\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "'''\n",
    "STEP 1: LOADING DATASET\n",
    "'''\n",
    "\n",
    "train_dataset = dsets.MNIST(root='./data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "'''\n",
    "STEP 2: MAKING DATASET ITERABLE\n",
    "'''\n",
    "\n",
    "batch_size = 100\n",
    "n_iters = 6000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "'''\n",
    "STEP 3: CREATE MODEL CLASS\n",
    "'''\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        # Linear function\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim) \n",
    "        # Non-linearity\n",
    "        self.relu = nn.ReLU()\n",
    "        # Linear function (readout)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Linear function\n",
    "        out = self.fc1(x)\n",
    "        # Non-linearity\n",
    "        out = self.relu(out)\n",
    "        # Linear function (readout)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "'''\n",
    "STEP 4: INSTANTIATE MODEL CLASS AND DEVICE\n",
    "'''\n",
    "input_dim = 28 * 28\n",
    "hidden_dim = 100\n",
    "output_dim = 10\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = SimpleModel(input_dim, hidden_dim, output_dim).to(device)\n",
    "\n",
    "'''\n",
    "STEP 5: INSTANTIATE LOSS CLASS\n",
    "'''\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "'''\n",
    "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
    "'''\n",
    "learning_rate = 0.1\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True)\n",
    "\n",
    "'''\n",
    "STEP 7: INSTANTIATE STEP LEARNING SCHEDULER CLASS\n",
    "'''\n",
    "# lr = lr * factor \n",
    "# mode='max': look for the maximum validation accuracy to track\n",
    "# patience: number of epochs - 1 where loss plateaus before decreasing LR\n",
    "        # patience = 0, after 1 bad epoch, reduce LR\n",
    "# factor = decaying factor\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=0, verbose=True)\n",
    "# multi-step\n",
    "# scheduler = MultiStepLR(optimizer, milestones=[50, 80, 100], gamma=0.1)  # reduce lr by x0.1 at epoch 50, 80 and 100\n",
    "\n",
    "'''\n",
    "STEP 7: TRAIN THE MODEL\n",
    "'''\n",
    "iteration = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Send images and labels to device\n",
    "        images = images.view(-1, 28 * 28).to(device)\n",
    "        labeles = labels.to(device)\n",
    "\n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        iteration += 1\n",
    "        \n",
    "        ''' Validation '''\n",
    "        if iteration % 500 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                # Send images and labels to device\n",
    "                images = images.view(-1, 28 * 28).to(device)\n",
    "                labeles = labels.to(device)\n",
    "\n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "\n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "\n",
    "                # Total correct predictions\n",
    "                # Without .item(), it is a uint8 tensor which will not work when you pass this number to the scheduler\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "            accuracy = 100 * correct / total\n",
    "\n",
    "            # Print Loss\n",
    "            # print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.data[0], accuracy))\n",
    "\n",
    "    # Decay Learning Rate, pass validation accuracy for tracking at every epoch\n",
    "    print('Epoch {} completed'.format(epoch))\n",
    "    print('Loss: {}. Accuracy: {}'.format(loss.item(), accuracy))\n",
    "    print('-' * 20)\n",
    "    scheduler.step(accuracy)  # accuracy is used to track down a plateau\n",
    "    # scheduler.step()  # for multi-step, no need to pass anything to the scheduler when advancing it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <img src=\"https://img.icons8.com/dusk/64/000000/wired-network.png\" style=\"height:50px;display:inline\"> Adaptive Learning Rate Methods\n",
    "---\n",
    "- **Adapative learning methods** compute individual learning rates for different parameters.\n",
    "- Previously, we performed an update for all parameters $w$ at once as every parameter $w_i$ used the same learning rate $\\alpha$.\n",
    "\n",
    "\n",
    "Popular algorithms include: AdaGrad, Rprop, RMSprop, Adam and more..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/cotton/64/000000/olympic-torch.png\" style=\"height:50px;display:inline\"> Optimizers in PyTorch\n",
    "---\n",
    "All optimizers mentioned in this tutorial and more can be imported from the `torch.optim` library. Check out the full list of available optimizers in <a href=\"https://pytorch.org/docs/stable/optim.html#algorithms\">this link</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<!-- ## <img src=\"https://img.icons8.com/plasticine/100/000000/horizontal-settings-mixer.png\" style=\"height:50px;display:inline\"> Adagrad\n",
    "---\n",
    "* **Adagrad**: one of the first adaptive learning rate algorithm, with the basic idea of adapting the learning rate to the parameters by performing smaller updates (i.e. low learning rates) for parameters associated with *frequently* occurring features, and larger updates (i.e. high learning rates) for parameters associated with *infrequent* features.\n",
    "    * For this reason, it works well with *sparse* data.\n",
    "* Adagrad uses a different learning rate for every parameter $w_i$ at every time step $k$.\n",
    "* We denote:\n",
    "    * $\\alpha$ - the learning rate.\n",
    "    * $g_k = \\nabla f(w^k)$, the gradient at time step $k$, and $g_{i, k}$ the *partial* derivative w.r.t. the parameter $w_i$ at time step $k$.\n",
    "    * $G_k \\in \\mathbb{R}^{d \\times d}$ - a *diagonal* matrix, where each element $G_{i,i}^k$ is the **sum of squares of the gradients w.r.t $w_i$ up to time step $k$**, $G_{i, i}^k = \\sum_{j=1}^k g_{i, j}^2$.\n",
    "    * $\\epsilon$, a \"smoothing\" term that prevents division by zero, deafult is $10^{-8}$, but can range from $10^{-4}$ to $10^{-8}$. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <img src=\"https://img.icons8.com/plasticine/100/000000/horizontal-settings-mixer.png\" style=\"height:50px;display:inline\"> Adagrad\n",
    "---\n",
    "* **Adagrad**: one of the first adaptive learning rate algorithm\n",
    "* Main idea: Adapt the learning rate to the parameters by \n",
    "    * smaller updates (i.e. low learning rates) for parameters associated with *frequently* occurring features\n",
    "    * larger updates (i.e. high learning rates) for parameters associated with *infrequent* features.\n",
    "    * For this reason, it works well with *sparse* data.\n",
    "* Adagrad uses a different learning rate for every parameter $w_i$ at every time step $k$.\n",
    "* We denote:\n",
    "    * $\\alpha$ - the learning rate.\n",
    "    * $g_k = \\nabla f(w^k)$, the gradient at time step $k$, and $g_{i, k}$ the *partial* derivative w.r.t. the parameter $w_i$ at time step $k$.\n",
    "    * $G_k \\in \\mathbb{R}^{d \\times d}$ - a *diagonal* matrix, where each element $G_{i,i}^k$ is the **sum of squares of the gradients w.r.t $w_i$ up to time step $k$**, $G_{i, i}^k = \\sum_{j=1}^k g_{i, j}^2$.\n",
    "    * $\\epsilon$, a \"smoothing\" term that prevents division by zero, deafult is $10^{-8}$, but can range from $10^{-4}$ to $10^{-8}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* The Adagrad update rule: $$ w_i^{k+1} = w_i^k -\\frac{\\alpha}{\\sqrt{G_{i,i}^k + \\epsilon}} \\cdot g_{i, k} $$\n",
    "* Interestingly, without the square root operation, the algorithm performs much worse.\n",
    "* In vectorized form, we use the matrix-vector product $\\odot$ between $G_k$ and $g_k$: $$ w^{k+1} = w^k -\\frac{\\alpha}{\\sqrt{G^k + \\epsilon}} \\odot g_{k} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* **Main advantage**: Alleviates the need to manually reduce the learning rate\n",
    "<!--     * Most implementations use a default value of 0.01 for the learning rate. -->\n",
    "* **Main weakness**: $G_k$ accumulator keeps growing during training (with positive quantities) \n",
    "    * This causes the learning rate to **shrink** and eventually become very small\n",
    "    * At some point the algorithm effectively stops training\n",
    "<!-- * **Main weakness**: $G_k$ is the accumulation of the squared gradients (a positive quantity) in the denominator which keeps growing during training and causes the learning rate to **shrink** and eventually become very small, at which point the algorithm doesn't acquire additional knowledge. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/cotton/64/000000/olympic-torch.png\" style=\"height:50px;display:inline\"> Adagrad in PyTorch\n",
    "---\n",
    "* `torch.optim.Adagrad(model.parameters(), lr=learning_rate, initial_accumulator_value=0, eps=1e-10)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <img src=\"https://img.icons8.com/cute-clipart/64/000000/square-root.png\" style=\"height:50px;display:inline\"> RMSprop\n",
    "---\n",
    "* **RMSprop**: an unpublished (no official paper) optimization algorithm designed for neural networks, first proposed by Geoffrey Hinton in <a href=\"https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf\">lecture 6 (slide 29)</a> of the online course â€œNeural Networks for Machine Learningâ€.\n",
    "* The RMSProp update adjusts the **Adagrad** method in a very simple way in an attempt to reduce its aggressive, monotonically decreasing learning rate.\n",
    "* In particular, it uses **a moving average of squared gradients** instead.\n",
    "* We denote:\n",
    "    * $\\alpha$ - the learning rate.\n",
    "    * $g_k = \\nabla f(w^k)$\n",
    "    * $\\mathbb{E}[g^2]$ - moving average of squared gradients (stored in a cache with squared gradients from previous iterations).\n",
    "    * $\\beta$ - moving average parameter (good default value â€” 0.9)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* The RMSprop update rule: $$ \\mathbb{E}[g^2]_{k+1} = \\beta\\mathbb{E}[g^2]_k + (1-\\beta)g_k^2 $$ $$ w^{k+1} = w^k -\\frac{\\alpha}{\\sqrt{\\mathbb{E}[g^2]_{k+1}}}\\nabla f(w^k) $$\n",
    "* The learning rate is adapted by dividing by the root of squared gradient, but since we only have the estimate of the gradient on the current mini-batch, we need instead to use the moving average of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/cotton/64/000000/olympic-torch.png\" style=\"height:50px;display:inline\"> RMSprop in PyTorch\n",
    "---\n",
    "* `torch.optim.RMSprop(model.parameters(), lr=learning_rate, alpha=0.99)`\n",
    "    * `alpha` is $\\beta$ from the equations above, the moving average parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <img src=\"https://img.icons8.com/clouds/100/000000/heat-map.png\" style=\"height:50px;display:inline\"> Adam - Adaptive Moment Estimation\n",
    "---\n",
    "* **Adam**: another optimization method that computes adaptive learning rates for each parameter. \n",
    "* Adam combines the advantages of Adagrad, RMSprop and Momentum: \n",
    "    * Use the **squared gradients to scale the learning rate** (like RMSprop)\n",
    "    * Use  **moving average of the gradient instead of the gradient itself** (like SGD with momentum)\n",
    "<!-- * In addition to storing an exponentially decaying average of past **squared gradients** like Adadelta and RMSprop, Adam also keeps an exponentially decaying average of past **gradients** similar to momentum. -->\n",
    "* Whereas momentum can be seen as a ball running down a slope, Adam behaves like a heavy ball with friction, and thus prefers flat minima in the error surface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "* We denote:\n",
    "    * $\\alpha$ - the learning rate.\n",
    "    * $m$ - moving average of gradients. Estimates the first moment (mean) of the gardients.\n",
    "    * $v$ - moving average of squared gradients. Estimates the second momemnt (variance) of the gradients.\n",
    "    * $\\beta_1$ - moving average parameter for $m$ (default: 0.9).\n",
    "    * $\\beta_2$ - moving average parameter for $v$ (default: 0.999)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* The Adam update rule: $$ \\mathbb{E}[g]_{k+1} = m_{k+1} = \\beta_1 m_k + (1-\\beta_1)\\nabla f(w^k) = \\beta_1 m_k + (1-\\beta_1)g_k $$  $$ \\mathbb{E}[g^2]_{k+1} = v_{k+1} = \\beta_2 v_k + (1-\\beta_2)(\\nabla f(w^k))^2 = \\beta_2 v_k + (1-\\beta_2)g^2_k $$ Then, we use an **unbiased** estimation: $$ \\hat{m}_{k+1} = \\frac{m_{k+1}}{1 -\\beta_1^{k+1}} $$ $$ \\hat{v}_{k+1} = \\frac{v_{k+1}}{1 -\\beta_2^{k+1}} $$ (the $\\beta$'s are taken with the power of the current iteration) $$ w^{k+1} = w^k -\\frac{\\alpha}{\\sqrt{\\hat{v}_{k+1}} +\\epsilon}\\hat{m}_{k+1} $$\n",
    "\n",
    "* $\\epsilon$, a \"smoothing\" term that prevents diviosn by zero, deafult's is $10^{-8}$, but can range from $10^{-4}$ to $10^{-8}$. Note that the default $\\epsilon = 1e-8$ might be sub-optimal for various tasks, and thus it is recommended to test other values $\\in [1e-3, 1e-12]$ as well.\n",
    "    *  <a href=\"https://arxiv.org/abs/2011.02150\">EAdam Optimizer: How Îµ Impact Adam</a> - Wei Yuan, Kai-Xin Gao.\n",
    "    *  <a href=\"http://zna.do/epsilon\">Îµ, a A Nuisance No More</a> - Zack Nado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/cotton/64/000000/olympic-torch.png\" style=\"height:50px;display:inline\"> Adam in PyTorch\n",
    "---\n",
    "* `torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999))`\n",
    "    \n",
    "If you want to use weight decay (i.e., $L_2$ regularization on the weights), use `AdamW`, which adds a fix to the update rule to account for the weight regularization.\n",
    "\n",
    "* <a href=\"https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html#torch.optim.AdamW\">`torch.optim.AdamW(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), weight_decay=0.01)`</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <img src=\"https://img.icons8.com/doodle/96/000000/scales--v1.png\" style=\"height:50px;display:inline\"> Comparison Between Methods\n",
    "---\n",
    "<center><img src=\"./assets/loss_conturs_all_grad_w_adam.gif\" style=\"height:500px\"></center>\n",
    "\n",
    "* Contours of a loss surface and time evolution of different optimization algorithms. \n",
    "* Notice the \"overshooting\" behavior of **momentum-based methods**, which makes the optimization look like a ball rolling down the hill.\n",
    "\n",
    "* <a href=\"https://github.com/ilguyi/optimizers.numpy\">Image Source</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"./assets/saddle_point_all_grad.gif\" style=\"height:250px\"></center>\n",
    "\n",
    "* A visualization of a **saddle point** in the optimization landscape, where the curvature along different dimension has different signs (one dimension curves up and another down). \n",
    "* Notice that SGD has a very hard time breaking symmetry and gets *stuck* on the top. \n",
    "* Conversely, algorithms such as RMSprop will see very low gradients in the saddle direction. Due to the denominator term in the RMSprop update, this will increase the effective learning rate along this direction, helping RMSProp proceed.\n",
    "\n",
    "* Image credit: <a href=\"https://twitter.com/alecrad\">Alec Radford</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <img src=\"https://img.icons8.com/bubbles/50/new.png\" style=\"height:50px;display:inline\"> Recent Advances in Optimizers\n",
    "---\n",
    "* Adam, \"the king of optimizers\", has been the go-to optimizer for some time now, and while there were several attempts to dethrone it, there was not a major breakthrough in the optimizers literature to replace it, and Adam is still widely used in all major deep learning domains.\n",
    "* However, several notable recent works have proposed alternatives that show great promise as candidates to replace Adam, which we cover below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <img src=\"https://img.icons8.com/cute-clipart/64/pray.png\" style=\"height:50px;display:inline\"> AdaBelief\n",
    "---\n",
    "* Introduced in <a href=\"https://arxiv.org/abs/2010.07468\">AdaBelief Optimizer: Adapting Stepsizes by the Belief in Observed Gradients</a> - Zhuang et al., NeurIPS 2020.\n",
    "* **Motivation**: achieve three goals -- fast convergence as in adaptive methods, good generalization as in SGD, and training stability.\n",
    "* **Main idea**: adapt the step-size according to the \"belief\" in the current gradient direction -- the moving average (EMA) of the noisy gradient serves as the prediction of the gradient at the next time step; take a small step if the prediction and current gradient have a large difference and otherwise take a large step.\n",
    "  \n",
    "<center><img src=\"./assets/adabelief_algo.png\" style=\"height:250px\"></center>\n",
    "\n",
    "<center><img src=\"./assets/adabelief2.gif\" style=\"height:300px\"></center>\n",
    "\n",
    "\n",
    "* <a href=\"https://juntang-zhuang.github.io/adabelief/\">Website</a>, <a href=\"https://github.com/juntang-zhuang/Adabelief-Optimizer\">PyTorch Code</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <img src=\"https://img.icons8.com/external-bzzricon-smooth-bzzricon-studio/64/external-mad-puppy-bzzricon-smooth-bzzricon-smooth-bzzricon-studio.png\" style=\"height:50px;display:inline\"> MADGRAD\n",
    "---\n",
    "* Introduced in <a href=\"https://arxiv.org/abs/2101.11075\">Adaptivity without Compromise: A Momentumized, Adaptive, Dual Averaged Gradient Method for Stochastic Optimization</a> - Aaron Defazio and Samy Jelassi (META AI Research), JMLR 2022.\n",
    "* **Motivation**: combine adaptivity with strong generalization performance.\n",
    "* **Main idea**: based the dual averaging formulation of AdaGrad, combined with momentum and adaptivity to achieve the generalization performance of SGD and the fast convergence of Adam. \n",
    "  \n",
    "<center><img src=\"./assets/madgrad_algo.PNG\" style=\"height:250px\"></center>\n",
    "\n",
    "* <a href=\"https://madgrad.readthedocs.io/en/latest/\">Website</a>, <a href=\"https://github.com/facebookresearch/madgrad\">PyTorch Code</a>.\n",
    "    * Notes: GPU-only. Typically, the same learning rate schedule that is used for SGD or Adam may be used. MADGRAD requires less weight decay than other methods, often as little as zero. Momentum values used for SGD or Adamâ€™s $\\beta_1$ should work here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <img src=\"https://img.icons8.com/external-goofy-color-kerismaker/96/external-Seesaw-momentum-science-goofy-color-kerismaker.png\" style=\"height:50px;display:inline\"> Adan\n",
    "---\n",
    "* Introduced in <a href=\"https://arxiv.org/abs/2208.06677\">Adan: Adaptive Nesterov Momentum Algorithm for Faster Optimizing Deep Models</a> - Xie et al., 2022.\n",
    "* **Motivation**: improve the deep learning model training speed by utilizing an accurate and stable estimation of the gradient moments.\n",
    "* **Main idea**: a new Nesterov momentum estimation (NME) method, which avoids the extra overhead of computing gradient at the extrapolation point and used to estimate the gradient's first-order and second-order moments in adaptive gradient algorithms for convergence acceleration.\n",
    "  \n",
    "<center><img src=\"./assets/adan_algo.PNG\" style=\"height:300px\"></center>\n",
    "\n",
    "<center><img src=\"./assets/adan_table.PNG\" style=\"height:300px\"></center>\n",
    "\n",
    "* <a href=\"https://github.com/sail-sg/Adan\">PyTorch Code</a>.\n",
    "    * Notes: Adan has a slightly higher GPU memory cost than Adam/AdamW on a single node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <img src=\"https://img.icons8.com/clouds/100/overtime.png\" style=\"height:50px;display:inline\"> Schedule-Free Optimization\n",
    "---\n",
    "* Introduced in <a href=\"https://arxiv.org/abs/2405.15682\">The Road Less Scheduled</a> - Defazio et al., 2024.\n",
    "* Presents two new optimizers: **Schedule-free SGD** and **Schedule-free Adam**, that do not require learning rate scheduling (but can optionally be combined with one).\n",
    "* **Motivation**: learning rate schedules that depend on a stopping-step $T$ greatly outperform schedulers that do not require specification of $T$.\n",
    "* **Main idea**: eliminating the need for $T$ by unifiying scheduling and iterate averaging with no additional hyper-parameters.\n",
    "  \n",
    "<center><img src=\"./assets/schedule_free_adam_algo.PNG\" style=\"height:300px\"></center>\n",
    "\n",
    "<center><img src=\"./assets/schedule_free_graphs.PNG\" style=\"height:300px\"></center>\n",
    "\n",
    "* <a href=\"https://github.com/facebookresearch/schedule_free\">PyTorch Code</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/cotton/64/000000/stop-sign.png\" style=\"height:50px;display:inline\"> Question time!\n",
    "---\n",
    "#### Exam question\n",
    "This question appeared in Spring 2024 moed B\n",
    "\n",
    "We have an infinitly differentiable function $f:\\mathbb{R}^d\\rightarrow\\mathbb{E}$ with a finite global minimum.\n",
    "\n",
    "We optimize $f$ via heavy ball momentum:\n",
    "$$\\begin{aligned} \n",
    "v(t) &= \\gamma v(t-1)+\\nabla f(w(t-1)))\\quad \\\\\n",
    "w(t) &= w(t-1)-\\eta v(t)\n",
    "\\end{aligned}$$\n",
    "where $v(0)=0$.\n",
    "\n",
    "We are told that near the stationary point, we have\n",
    "$$f(w)=\\frac{1}{2}w^THw$$\n",
    "for $H$ the Hessian matrix.\n",
    "\n",
    "1. Use the Hessian eigenvector decomposition $H=U\\Lambda U^T$ for diagonal eingenvalue matrix $\\Lambda$ with eigenvalues $\\lambda_i$ and eigenvector matrix $U$, to show that the update rule can be written as $$\\left [ \\begin{array}{l}\n",
    "        q_i(t)\\\\z_i(t)\\\\\\end{array}\\right ]=A_i \\left [ \\begin{array}{l}\n",
    "        q_i(t-1)\\\\z_i(t-1)\\\\\\end{array}\\right ]$$ where $q=U^Tv,\\quad z=U^T w$. What is $A_i$?\n",
    "        \n",
    "2. We decompose $$A_i=V_i\\Sigma_i V_i^{-1}$$ where $\\Sigma_i$ is a diagonal eigenvalue matrix with eigenvalues $\\sigma_{i,\\pm}$. Show that\n",
    "   $$\\sigma_{i,\\pm}=\\frac{1}{2}\\left (1-\\eta\\lambda_i+\\gamma\\pm \\sqrt{(1-\\eta\\lambda_i+\\gamma)^2-4\\gamma}\\right )$$\n",
    "3. Write the solution for section 1 $h_i(t)=[q_i(t),z_i(t)]^T\\in\\mathbb{R}^2$ in terms of $h_i(0),V_i,\\Sigma_i,V_i^{-1}$ and explain why the convergence rate is $$\\mathrm{rate}=\\max_i\\max_{\\pm}|\\sigma_{i,\\pm}|$$ and the requirement for convergence is $\\mathrm{rate}<1$.\n",
    "\n",
    "Hint for the next sections: $$f_{\\pm}(x)=|x\\pm\\sqrt{x^2-c^2}|$$ is monotonically increasing/decreasing when $|x|>|c|$ and constant if $|x|<|c|$.\n",
    "\n",
    "4. Calculate $|\\sigma_{i,\\pm}|$ for complex/imaginary eigenvalues. What should $\\gamma$ be such that $|\\sigma_{i,\\pm}|<1$?\n",
    "5. For the $\\gamma$ value in section 4, we increase $\\eta$ such that $\\mathrm{rate}\\rightarrow 1$. Let $\\lambda_{\\max}$ be the largest eigenvalue of $H$. show that $$\\mathrm{rate}=\\max_i\\max_{\\pm}|\\sigma_{i,\\pm}|=\\frac{1}{2}\\left |1-\\eta\\lambda_{\\max}+\\gamma- \\sqrt{(1-\\eta\\lambda_{\\max}+\\gamma)^2-4\\gamma}\\right |$$\n",
    "6. Under the conditions of section 5, show that convergence requires $$\\eta<\\frac{2(1+\\gamma)}{\\lambda_{\\max}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <img src=\"https://img.icons8.com/cotton/64/000000/checkmark.png\" style=\"height:50px;display:inline\"> Answer time!\n",
    "---\n",
    "<details>\n",
    "  <summary>Hint</summary>\n",
    "    \n",
    "1. Similar to the non-momentum setting.\n",
    "2. Find the characterisitc polynomial and solve the quadratic equation for its roots.\n",
    "3. Show that for some diagonal matrix $D$, $h(t)=Dh(0)$, the convergence rate depends on the eigenvalue closest to $1$ in absolute value.\n",
    "4. The complex eigenvalues are when the square root is negative. Consider $|\\sigma_{i,\\pm}|$ when this happens.\n",
    "5. Consider which of $|\\sigma_{i,+}|,|\\sigma_{i,-}|$ is larger.\n",
    "6. Note that the rate is monotonic, use the expression for we are given for $\\eta$.\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Section 1\n",
    "<span style=\"color:gray\">\n",
    "Use the Hessian eigenvector decomposition $H=U\\Lambda U^T$ for diagonal eingenvalue matrix $\\Lambda$ with eigenvalues $\\lambda_i$ and eigenvector matrix $U$, to show that the update rule can be written as $$\\left [ \\begin{array}{l}\n",
    "        q_i(t)\\\\z_i(t)\\\\\\end{array}\\right ]=A_i \\left [ \\begin{array}{l}\n",
    "        q_i(t-1)\\\\z_i(t-1)\\\\\\end{array}\\right ]$$ where $q=U^Tv,\\quad z=U^T w$. What is $A_i$?\n",
    "</span>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- We note that $U^TU=I$ -->\n",
    "We start with finding the update rule in the roatated weight space:\n",
    "\n",
    "$$v(t) = \\gamma v(t-1)+\\nabla f(w(t-1)))$$\n",
    "$$q=U^Tv,\\quad z=U^T w$$\n",
    "\n",
    "\n",
    "$$\\begin{aligned}\n",
    "q(t) &= U^T v(t) \\\\\n",
    "&= \\gamma U^Tv(t-1)+U^THw(t-1) \\\\\n",
    "&= \\gamma q(t-1)+U^TU\\Lambda Uw(t-1) \\\\ \n",
    "&=\\gamma q(t-1)+\\Lambda z(t-1) \\\\\\\\\n",
    "z(t) &= U^T w(t) \\\\\n",
    "&=U^Tw(t-1)-\\eta U^Tv(t)\\\\\n",
    "&=z(t-1)-\\eta q(t)\n",
    "\\end{aligned}$$\n",
    "\n",
    "<!-- Decomposing per $0\\leq i\\leq d$ we have  -->\n",
    "\n",
    "Since $\\Lambda$ is diagonal, we can decompose per $0\\leq i\\leq d$:\n",
    "$$\\begin{aligned} \n",
    "q_i(t) &= \\gamma q_i(t-1)+\\lambda_i z_i(t-1) \\\\\\\\\n",
    "z_i(t) &= z_i(t-1)-\\eta q_i(t) \\\\\n",
    "&= z_i(t-1)-\\eta (\\gamma q_i(t-1)+\\lambda_i z_i(t-1)) \\\\\n",
    "&= (1-\\eta\\lambda_i)z_i(t-1)-\\eta\\gamma q_i(t-1)\n",
    "\\end{aligned}$$\n",
    "\n",
    "Putting everything together we get \n",
    "$$\\left ( \\begin{array}{l}q_i(t)\\\\z_i(t)\\\\\\end{array}\\right )=\\left ( \\begin{array}{cc}\\gamma & \\lambda_i\\\\-\\eta\\gamma & 1-\\eta\\lambda_i\\\\\\end{array}\\right )\\left ( \\begin{array}{l}q_i(t-1)\\\\z_i(t-1)\\\\\\end{array}\\right )$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Section 2\n",
    "<span style=\"color:gray\">\n",
    "We decompose $$A_i=V_i\\Sigma_i V_i^{-1}$$ where $\\Sigma_i$ is a diagonal eigenvalue matrix with eigenvalues $\\sigma_{i,\\pm}$. \n",
    "    \n",
    "Show that\n",
    "   $$\\sigma_{i,\\pm}=\\frac{1}{2}\\left (1-\\eta\\lambda_i+\\gamma\\pm \\sqrt{(1-\\eta\\lambda_i+\\gamma)^2-4\\gamma}\\right )$$\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the characteristic polynomial, $$|A_i-\\sigma I|=0\\Rightarrow \\left | \\begin{array}{cc}\\gamma-\\sigma & \\lambda_i\\\\-\\eta\\gamma & 1-\\eta\\lambda_i-\\sigma\\\\\\end{array}\\right |=0$$\n",
    "$$(\\gamma-\\sigma)(1-\\eta\\lambda_i-\\sigma)-\\lambda_i(-\\eta\\gamma)=0$$\n",
    "$$\\Rightarrow \\sigma^2-(1-\\eta\\lambda_i+\\gamma)\\sigma+\\gamma=0$$\n",
    "Solving the quadratic equation gives us \n",
    "$$\\sigma_{i,\\pm}=\\frac{1}{2}\\left (1-\\eta\\lambda_i+\\gamma\\pm \\sqrt{(1-\\eta\\lambda_i+\\gamma)^2-4\\gamma}\\right )$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Section 3\n",
    "<span style=\"color:gray\">\n",
    "Write the solution for section 1 $h_i(t)=[q_i(t),z_i(t)]^T\\in\\mathbb{R}^2$ in terms of $h_i(0),V_i,\\Sigma_i,V_i^{-1}$ and explain why the convergence rate is $$\\mathrm{rate}=\\max_i\\max_{\\pm}|\\sigma_{i,\\pm}|$$ and the requirement for convergence is $\\mathrm{rate}<1$.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Denote $$ h_i=[q_i,z_i]^T, h=(h_1^T,h_2^T,\\ldots,h_d^T)^T \\,.$$\n",
    "\n",
    "Similarly to section 1, we would like to rotate $h_i$ such that we multiply by a diagonal matrix.\n",
    "\n",
    "To do that, we take $$ p_i=V_i^{-1}h_i \\,.$$\n",
    "We can show that\n",
    "\n",
    "$$\\begin{aligned} \n",
    "h_i(t) &= A_i h_i(t-1) \\\\\n",
    "\\Rightarrow p_i(t) &= V_i^{-1}h_i(t) \\\\\n",
    "&= V_i^{-1}V_i\\Sigma_i V_i^{-1} h_i(t-1)\\\\\n",
    "&= \\Sigma_i p_i(t-1)\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering $p_{i,\\pm}$ the two elements of $p_i$, we therefore have\n",
    "$$\\begin{aligned} \n",
    "p_{i,\\pm}(t) &= \\sigma_{i,\\pm} p_{i,\\pm}(t-1) \\\\\n",
    "\\Rightarrow p_{i,\\pm}(t) &= \\sigma_{i,\\pm}^t p_{i,\\pm}(0) \\\\\n",
    "\\Rightarrow p_{i}(t) &= \\Sigma_i^t p_i(0)\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{aligned} \\Rightarrow  h_i(t) &= V_i p_i(t) \\\\\n",
    "&= V_i \\Sigma_i^t p_i(0) \\\\\n",
    "&= V_i\\Sigma_i^t V_i^{-1}h_i(0) \n",
    "\\end{aligned}$$\n",
    "\n",
    "To get a general expression for $h(t)$ we gather these expression in a block diagonal matrix\n",
    "\n",
    "$$ h(t)=\\left ( \\begin{array}{ccc}V_1\\Sigma_1^tV_1^{-1}&0&0  \\\\ 0 & \\ddots& 0\\\\0&0&V_d\\Sigma_d^tV_d^{-1}\\\\\\end{array}\\right )h(0)$$\n",
    "\n",
    "The convergence rate depends on the eigenvalue whose absolute value is closest to $1$, as the slowest eigenvalue to converge, so \n",
    "$$\\mathrm{rate}=\\max_i\\max_{\\pm}|\\sigma_{i,\\pm}|$$\n",
    "and convegence requires $\\mathrm{rate}<1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Marking $h_i=[q_i,z_i]^T, h=(h_1^T,h_2^T,\\ldots,h_d^T)^T$ and $p_i=V_i^{-1}h_i$, we can show that\n",
    "$$h_i(t)=A_i h_i(t-1)\\Rightarrow p_i(t)=V_i^{-1}h_i(t)=V_i^{-1}V_i\\Sigma_i V_i^{-1} h_i(t-1)=\\Sigma_i p_i(t-1)$$\n",
    "Considering $p_{i,\\pm}$ the two elements of $p_i$, we therefor have\n",
    "$$p_{i,\\pm}(t)=\\sigma_{i,\\pm} p_{i,\\pm}(t-1)\\Rightarrow p_{i,\\pm}(t)=\\sigma_{i,\\pm}^t p_{i,\\pm}(0)$$\n",
    "$$\\Rightarrow  h_i(t)=V_i\\Sigma_i^t V_i^{-1}h_i(0)\\Rightarrow h_i(t)=\\left ( \\begin{array}{ccc}V_1\\Sigma_1^tV_1^{-1}&0&0  \\\\ 0 & \\ddots& 0\\\\0&0&V_d\\Sigma_d^tV_d^{-1}\\\\\\end{array}\\right )h_i(0)$$\n",
    "\n",
    "The convergence rate depends on the eigenvalue whose absolute value is closest to $1$, as the slowest eigenvalue to converge, so \n",
    "$$\\mathrm{rate}=\\max_i\\max_{\\pm}|\\sigma_{i,\\pm}|$$\n",
    "and convegence requires $\\mathrm{rate}<1$ -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Section 4\n",
    "<span style=\"color:gray\">\n",
    "Calculate $|\\sigma_{i,\\pm}|$ for complex/imaginary eigenvalues. What should $\\gamma$ be such that $|\\sigma_{i,\\pm}|<1$?\n",
    "\n",
    "Hint: $$f_{\\pm}(x)=|x\\pm\\sqrt{x^2-c^2}|$$ is monotonically increasing/decreasing when $|x|>|c|$ and constant if $|x|<|c|$.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complex eigenvalues imply that the value under the square root is negative, $(1-\\eta\\lambda_i+\\gamma)^2<4\\gamma$.\n",
    "$$|\\sigma_{i,\\pm}|=\\frac{1}{2}\\left | 1-\\eta\\lambda_i+\\gamma\\pm \\sqrt{(1-\\eta\\lambda_i+\\gamma)^2-(2\\sqrt{\\gamma})^2)}\\right |$$\n",
    "From the hint, $f_{\\pm}=\\left |1-\\eta\\lambda_i+\\gamma\\pm \\sqrt{(1-\\eta\\lambda_i+\\gamma)^2-(2\\sqrt{\\gamma})^2)}\\right |$ is constant in our range and it is easy to see that this constant is $|c|$ in general, or in our case\n",
    "$$|\\sigma_{i,\\pm}|=\\frac{1}{2}|2\\sqrt{\\gamma} |=\\sqrt{\\gamma}$$\n",
    "Notably, this does not depend on the learning rate $\\eta$ or on the eigenvalues of the Hessian $\\lambda_i$.\n",
    "Convergence requires $$\\sqrt{\\gamma}<1\\Rightarrow \\gamma<1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 5\n",
    "<span style=\"color:gray\">\n",
    "For the $\\gamma$ value in section 4, we increase $\\eta$ such that $\\mathrm{rate}\\rightarrow 1$. Let $\\lambda_{\\max}$ be the largest eigenvalue of $H$. show that $$\\mathrm{rate}=\\max_i\\max_{\\pm}|\\sigma_{i,\\pm}|=\\frac{1}{2}\\left |1-\\eta\\lambda_{\\max}+\\gamma- \\sqrt{(1-\\eta\\lambda_{\\max}+\\gamma)^2-4\\gamma}\\right |$$\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For large learning rate $\\eta$, $x=1-\\eta\\lambda_i+\\gamma$ is increasingly more negative, meaning \n",
    "$$\\sigma_{i,+}=\\frac{1}{2}\\left (1-\\eta\\lambda_i+\\gamma+\\sqrt{(1-\\eta\\lambda_i+\\gamma)^2-4\\gamma}\\right )$$ decreases while \n",
    "$$\\sigma_{i,-}=\\frac{1}{2}\\left (1-\\eta\\lambda_i+\\gamma- \\sqrt{(1-\\eta\\lambda_i+\\gamma)^2-4\\gamma}\\right )$$ increases.\n",
    "Since they both start from the positive value $\\sqrt{\\gamma}$, this means $|\\sigma_{i,-}|$ will be the larget of the eigenvalues, and since it is monotonically decreasing in $x$, we can choose $\\lambda_i=\\lambda_{\\max}$ to get the largest $|\\sigma_{i,-}|$.\n",
    "$$|\\sigma_{\\max,-}|=\\frac{1}{2}\\left |1-\\eta\\lambda_{\\max}+\\gamma- \\sqrt{(1-\\eta\\lambda_{\\max}+\\gamma)^2-4\\gamma}\\right |<1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 6\n",
    "<span style=\"color:gray\">\n",
    "Under the conditions of section 5, show that convergence requires $$\\eta<\\frac{2(1+\\gamma)}{\\lambda_{\\max}}$$\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x-\\sqrt{x^2-c^2}$ is monotonic, so we can just set $\\eta<\\frac{2(1+\\gamma)}{\\lambda_{\\max}}$ and check that the rate is less than $1$:\n",
    "$$\\mathrm{rate}<\\frac{1}{2}\\left |1-2(1+\\gamma)+\\gamma- \\sqrt{(1-2(1+\\gamma)+\\gamma)^2-4\\gamma}\\right |=\\frac{1}{2}\\left |-(1+\\gamma)- \\sqrt{(1+\\gamma)^2-4\\gamma}\\right |=\\frac{1}{2}\\left |-(1+\\gamma)- \\sqrt{((1-\\gamma))^2}\\right |$$\n",
    "$$\\mathrm{rate}<=\\frac{1}{2}\\left |-(1+\\gamma)- (1-\\gamma)\\right |=1$$\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ### <img src=\"https://img.icons8.com/cotton/64/000000/checkmark.png\" style=\"height:50px;display:inline\"> Answer time!\n",
    "---\n",
    "<details>\n",
    "  <summary>Hint</summary>\n",
    "    \n",
    "1. Similar to the non-momentum setting.\n",
    "2. Find the characterisitc polynomial and solve the quadratic equation for its roots.\n",
    "3. Show that for some diagonal matrix $D$, $h(t)=Dh(0)$, the convergence rate depends on the eigenvalue closest to $1$ in absolute value.\n",
    "4. The complex eigenvalues are when the square root is negative. Consider $|\\sigma_{i,\\pm}|$ when this happens.\n",
    "5. Consider which of $|\\sigma_{i,+}|,|\\sigma_{i,-}|$ is larger.\n",
    "6. Note that the rate is monotonic, use the expression for we are given for $\\eta$.\n",
    "    \n",
    "</details>\n",
    "\n",
    "<details>\n",
    "  <summary>Solution</summary>\n",
    "    \n",
    "#### Section 1\n",
    "We note that $U^TU=I$\n",
    "$$q(t)=U^T v(t)=\\gamma U^Tv(t-1)+U^THw(t-1)=\\gamma q(t-1)+U^TU\\Lambda Uw(t-1)=\\gamma q(t-1)+\\Lambda z(t-1)$$\n",
    "Decomposing per $0\\leq i\\leq d$ we have \n",
    "$$q_i(t)=\\gamma q_i(t-1)+\\lambda_i z_i(t-1)$$\n",
    "\n",
    "$$z(t)=U^T w(t)=U^Tw(t-1)-\\eta U^Tv(t)=z(t-1)-\\eta q(t)$$\n",
    "Decomposing per $0\\leq i\\leq d$ we have \n",
    "$$z(t)=z_i(t-1)-\\eta q_i(t)=z_i(t-1)-\\eta (\\gamma q_i(t-1)+\\lambda_i z_i(t-1))=(1-\\eta\\lambda_i)z_i(t-1)-\\eta\\gamma q_i(t-1)$$\n",
    "\n",
    "Putting everything together we get \n",
    "$$\\left ( \\begin{array}{l}q_i(t)\\\\z_i(t)\\\\\\end{array}\\right )=\\left ( \\begin{array}{cc}\\gamma & \\lambda_i\\\\-\\eta\\gamma & 1-\\eta\\lambda_i\\\\\\end{array}\\right )\\left ( \\begin{array}{l}q_i(t-1)\\\\z_i(t-1)\\\\\\end{array}\\right )$$\n",
    "\n",
    "#### Section 2\n",
    "Looking at the characteristic polynomial, $$|A_i-\\sigma I|=0\\Rightarrow \\left | \\begin{array}{cc}\\gamma-\\sigma & \\lambda_i\\\\-\\eta\\gamma & 1-\\eta\\lambda_i-\\sigma\\\\\\end{array}\\right |=0$$\n",
    "$$(\\gamma-\\sigma)(1-\\eta\\lambda_i-\\sigma)-\\lambda_i(-\\eta\\gamma)=0$$\n",
    "$$\\Rightarrow \\sigma^2-(1-\\eta\\lambda_i+\\gamma)\\sigma+\\gamma=0$$\n",
    "Solving the quadratic equation gives us \n",
    "$$\\sigma_{i,\\pm}=\\frac{1}{2}\\left (1-\\eta\\lambda_i+\\gamma\\pm \\sqrt{(1-\\eta\\lambda_i+\\gamma)^2-4\\gamma}\\right )$$\n",
    "\n",
    "#### Section 3\n",
    "Marking $h_i=[q_i,z_i]^T, h=(h_1^T,h_2^T,\\ldots,h_d^T)^T$ and $p_i=V_i^{-1}h_i$, we can show that\n",
    "$$h_i(t)=A_i h_i(t-1)\\Rightarrow p_i(t)=V_i^{-1}h_i(t)=V_i^{-1}V_i\\Sigma_i V_i^{-1} h_i(t-1)=\\Sigma_i p_i(t-1)$$\n",
    "Considering $p_{i,\\pm}$ the two elements of $p_i$, we therefor have\n",
    "$$p_{i,\\pm}(t)=\\sigma_{i,\\pm} p_{i,\\pm}(t-1)\\Rightarrow p_{i,\\pm}(t)=\\sigma_{i,\\pm}^t p_{i,\\pm}(0)$$\n",
    "$$\\Rightarrow  h_i(t)=V_i\\Sigma_i^t V_i^{-1}h_i(0)\\Rightarrow h_i(t)=\\left ( \\begin{array}{ccc}V_1\\Sigma_1^tV_1^{-1}&0&0  \\\\ 0 & \\ddots& 0\\\\0&0&V_d\\Sigma_d^tV_d^{-1}\\\\\\end{array}\\right )h_i(0)$$\n",
    "\n",
    "The convergence rate depends on the eigenvalue whose absolute value is closest to $1$, as the slowest eigenvalue to converge, so \n",
    "$$\\mathrm{rate}=\\max_i\\max_{\\pm}|\\sigma_{i,\\pm}|$$\n",
    "and convegence requires $\\mathrm{rate}<1$\n",
    "\n",
    "#### Section 4\n",
    "Complex eigenvalues imply that the value under the square root is negative, $(1-\\eta\\lambda_i+\\gamma)^2<4\\gamma$.\n",
    "$$|\\sigma_{i,\\pm}|=\\frac{1}{2}\\left | 1-\\eta\\lambda_i+\\gamma\\pm \\sqrt{(1-\\eta\\lambda_i+\\gamma)^2-(2\\sqrt{\\gamma})^2)}\\right |$$\n",
    "From the hint, $f_{\\pm}=\\left |1-\\eta\\lambda_i+\\gamma\\pm \\sqrt{(1-\\eta\\lambda_i+\\gamma)^2-(2\\sqrt{\\gamma})^2)}\\right |$ is constant in our range and it is easy to see that this constant is $|c|$ in general, or in our case\n",
    "$$|\\sigma_{i,\\pm}|=\\frac{1}{2}|2\\sqrt{\\gamma} |=\\sqrt{\\gamma}$$\n",
    "Notably, this does not depend on the learning rate $\\eta$ or on the eigenvalues of the Hessian $\\lambda_i$.\n",
    "Convergence requires $$\\sqrt{\\gamma}<1\\Rightarrow \\gamma<1$$\n",
    "\n",
    "#### Section 5\n",
    "For large learning rate $\\eta$, $x=1-\\eta\\lambda_i+\\gamma$ is increasingly more negative, meaning \n",
    "$$\\sigma_{i,+}=\\frac{1}{2}\\left (1-\\eta\\lambda_i+\\gamma+\\sqrt{(1-\\eta\\lambda_i+\\gamma)^2-4\\gamma}\\right )$$ decreases while \n",
    "$$\\sigma_{i,-}=\\frac{1}{2}\\left (1-\\eta\\lambda_i+\\gamma- \\sqrt{(1-\\eta\\lambda_i+\\gamma)^2-4\\gamma}\\right )$$ increases.\n",
    "Since they both start from the positive value $\\sqrt{\\gamma}$, this means $|\\sigma_{i,-}|$ will be the larget of the eigenvalues, and since it is monotonically decreasing in $x$, we can choose $\\lambda_i=\\lambda_{\\max}$ to get the largest $|\\sigma_{i,-}|$.\n",
    "$$|\\sigma_{\\max,-}|=\\frac{1}{2}\\left |1-\\eta\\lambda_{\\max}+\\gamma- \\sqrt{(1-\\eta\\lambda_{\\max}+\\gamma)^2-4\\gamma}\\right |<1$$\n",
    "\n",
    "#### Section 6\n",
    "$x-\\sqrt{x^2-c^2}$ is monotonic, so we can just set $\\eta<\\frac{2(1+\\gamma)}{\\lambda_{\\max}}$ and check that the rate is less than $1$:\n",
    "$$\\mathrm{rate}<\\frac{1}{2}\\left |1-2(1+\\gamma)+\\gamma- \\sqrt{(1-2(1+\\gamma)+\\gamma)^2-4\\gamma}\\right |=\\frac{1}{2}\\left |-(1+\\gamma)- \\sqrt{(1+\\gamma)^2-4\\gamma}\\right |=\\frac{1}{2}\\left |-(1+\\gamma)- \\sqrt{((1-\\gamma))^2}\\right |$$\n",
    "$$\\mathrm{rate}<=\\frac{1}{2}\\left |-(1+\\gamma)- (1-\\gamma)\\right |=1$$\n",
    "</details> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/video-playlist.png\" style=\"height:50px;display:inline\"> Recommended Videos\n",
    "---\n",
    "#### <img src=\"https://img.icons8.com/cute-clipart/64/000000/warning-shield.png\" style=\"height:30px;display:inline\"> Warning!\n",
    "* These videos do not replace the lectures and tutorials.\n",
    "* Please use these to get a better understanding of the material, and not as an alternative to the written material.\n",
    "\n",
    "#### Video By Subject\n",
    "\n",
    "* Gradient Descent - <a href=\"https://www.youtube.com/watch?v=sDv4f4s2SB8\">Gradient Descent, Step-by-Step</a>\n",
    "    * <a href=\"https://www.youtube.com/watch?v=jc2IthslyzM\">Mathematics of Gradient Descent - Intelligence and Learning</a>\n",
    "* Stochastic Gradient Descent - <a href=\"https://www.youtube.com/watch?v=vMh0zPT0tLI\">Stochastic Gradient Descent, Clearly Explained</a>\n",
    "* Momentum - <a href=\"https://www.youtube.com/watch?v=k8fTYJPd3_I\">Gradient Descent With Momentum (C2W2L06)</a>\n",
    "* RMSProp - <a href=\"https://www.youtube.com/watch?v=_e-LFe_igno\">RMSProp (C2W2L07)</a>\n",
    "* Adam - <a href=\"https://www.youtube.com/watch?v=JXQT_vxqwIs\">Adam Optimization Algorithm (C2W2L08)</a>\n",
    "* Learning Rate Decay - <a href=\"https://www.youtube.com/watch?v=QzulmoOg2JE\">Learning Rate Decay (C2W2L09)</a>\n",
    "* Momentum, Adagrad, RMSProp, Adam - <a href=\"https://www.youtube.com/watch?v=gmwxUy7NYpA\">UC Berkeley, STAT 157 - Momentum, Adagrad, RMSProp, Adam</a>\n",
    "* AdaBelief - <a href=\"https://www.youtube.com/watch?v=oGH7dmwvuaY\">AdaBelief Optimizer: Theory and Practical Guidelines</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## <img src=\"https://img.icons8.com/dusk/64/000000/prize.png\" style=\"height:50px;display:inline\"> Credits\n",
    "---\n",
    "* Icons made by <a href=\"https://www.flaticon.com/authors/becris\" title=\"Becris\">Becris</a> from <a href=\"https://www.flaticon.com/\" title=\"Flaticon\">www.flaticon.com</a>\n",
    "* Icons from <a href=\"https://icons8.com/\">Icons8.com</a> - https://icons8.com\n",
    "* Datasets from <a href=\"https://www.kaggle.com/\">Kaggle</a> - https://www.kaggle.com/\n",
    "* Examples and code snippets were taken from <a href=\"http://shop.oreilly.com/product/0636920052289.do\">\"Hands-On Machine Learning with Scikit-Learn and TensorFlow\"</a>\n",
    "* <a href=\"https://cs231n.github.io/neural-networks-3/\">CS231n: Convolutional Neural Networks for Visual Recognition</a>\n",
    "* <a href=\"https://www.deeplearningwizard.com/deep_learning/boosting_models_pytorch/lr_scheduling/\">Deep Learning Wizard -Learning Rate Scheduling</a>\n",
    "* <a href=\"https://dominikschmidt.xyz/nesterov-momentum\">Understanding Nesterov Momentum (NAG)</a>\n",
    "* <a href=\"https://ruder.io/optimizing-gradient-descent/\">Sebastian Ruder - An overview of gradient descent optimization algorithms</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
